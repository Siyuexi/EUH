EventId,EventTemplate,Occurrences
30881201,nohup ignoring input<*>,3
64ca1ffb,<*> Start obtain and cache Kerberos ticket<*>granting ticket <*><*>,3
58edc19e,Password for team7@BIGDATA <*>,3
457d0ad2,<*> Start execute hibench test <*><*>,3
57ba98a2,mkdir cannot create directory <*> File exists<*>,34
4f64db05,Start changing datasize ...<*>,3
370ddbe5,Creating <*> data ...<*>,5
34eba875,patching args<*><*>,32
2cb7bf92,Parsing conf <*>,128
64dce50b,probe sleep jar <*>,32
85e07766,ERROR execute cmd '( <*> node <*>list <*>> <*> <*> grep RUNNING )' timedout.<*> STDOU<*> STDER<*> Please check!<*>,1
bb05e6a0,"Traceback (most recent call last) <*> File ""<*> line <*> in <module><*> load_config(conf_root workload_configFile workload_folder patching_config)<*> File ""<*> line <*> in load_config<*> generate_optional_value()<*> File ""<*> line <*> in generate_optional_value<*> probe_masters_slaves_hostnames()<*> File ""<*> line <*> in probe_masters_slaves_hostnames<*> probe_masters_slaves_by_Yarn()<*> File ""<*> line <*> in probe_masters_slaves_by_Yarn<*> assert <*> ""Get workers from yarn<*>site.xml page failed reason %s\nplease set <*> and <*> manually"" % e<*> Get workers from yarn<*>site.xml page failed reason ( <*> node <*>list <*>> <*> <*> grep RUNNING ) executed timedout for <*> seconds<*>",1
7740a8ad,please set <*> and <*> manually<*>,1
9bcfb7fc,<*> line <*> . filename argument required<*>,1
9d29786a,start HadoopPrepareTerasort bench<*>,2
1e6c0489,hdfs rm <*>r <*> <*>config <*> fs <*>rm <*>r <*>skipTrash hdfs <*>,31
ab84931e,Deleted hdfs <*>,30
d9bfd3e9,Submit MapReduce Job <*> <*>config <*> jar <*> teragen <*>D <*> <*>D <*> <*>D <*> <*> hdfs <*>,2
b90b8028,22<*> <*> <*> <*> INFO <*> Connecting to Application History server at <*>,51
24ceeb34,22<*> <*> <*> <*> INFO <*> Created token for team7 HDFS_DELEGATION_TOKEN owner<*>team7@BIGDATA renewer<*>yarn realUser<*> issueDate<*><*> maxDate<*><*> sequenceNumber<*><*> masterKeyId<*><*> on ha<*>hdfs cuhkcluster<*>,44
5a8c1ed1,22<*> <*> <*> <*> INFO <*> Getting new token from http <*> renewer rm<*>,44
839fd495,22<*> <*> <*> <*> INFO <*> New token received (Kind kms<*>dt Service <*> Ident (kms<*>dt owner<*>team7 renewer<*>yarn realUser<*> issueDate<*><*> maxDate<*><*> sequenceNumber<*><*> masterKeyId<*><*>))<*>,44
9f34b013,22<*> <*> <*> <*> INFO <*> Got dt for hdfs <*> Kind <*> Service <*> <*> Ident <*> <*> <*> <*> <*> renewer<*>yarn realUser<*> issueDate<*><*> maxDate<*><*> sequenceNumber<*><*> masterKeyId<*><*>)<*>,75
f8e8b293,22<*> <*> <*> <*> INFO <*> Disabling Erasure Coding for path <*>,44
62bef29e,22<*> <*> <*> <*> INFO terasort.TeraGen Generating <*> using <*><*>,2
19777195,22<*> <*> <*> <*> INFO <*> number of splits <*><*>,42
91345cb3,22<*> <*> <*> <*> INFO <*> <*> <*> <*> job job_<*>_<*><*>,85
d470628a,22<*> <*> <*> <*> INFO <*> Executing with tokens [Kind HDFS_DELEGATION_TOKEN Service ha<*>hdfs cuhkcluster Ident (token for team7 HDFS_DELEGATION_TOKEN owner<*>team7@BIGDATA renewer<*>yarn realUser<*> issueDate<*><*> maxDate<*><*> sequenceNumber<*><*> masterKeyId<*><*>) Kind kms<*>dt Service <*> Ident (kms<*>dt owner<*>team7 renewer<*>yarn realUser<*> issueDate<*><*> maxDate<*><*> sequenceNumber<*><*> masterKeyId<*><*>)]<*>,42
3191cd9c,22<*> <*> <*> <*> INFO <*> found resource resource<*>types.xml at file <*>,32
d7d8e17f,22<*> <*> <*> <*> INFO <*> Timeline service address <*>,43
96c7c602,22<*> <*> <*> <*> INFO <*> Submitted application application_<*>_<*><*>,42
d0cd39a6,22<*> <*> <*> <*> INFO <*> The url to track the job http <*>,43
953c392e,22<*> <*> <*> <*> INFO mapreduce.Job Job job_<*>_<*> <*>,62
dfa2bb5d,22<*> <*> <*> <*> INFO <*> map <*>% reduce <*>%<*>,258
b924863e,22<*> <*> <*> <*> INFO mapreduce.Job Counters <*><*> File System Counters<*> FILE Number of bytes read<*><*><*> FILE Number of bytes written<*><*><*> FILE Number of read operations<*><*><*> FILE Number of large read operations<*><*><*> FILE Number of write operations<*><*><*> HDFS Number of bytes read<*><*><*> HDFS Number of bytes written<*><*><*> HDFS Number of read operations<*><*><*> HDFS Number of large read operations<*><*><*> HDFS Number of write operations<*><*><*> Job Counters <*> Launched map tasks<*><*><*> Other local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Input split bytes<*><*><*> Spilled Records<*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*> <*> CHECKSUM<*><*><*> File Input Format Counters <*> Bytes Read<*><*><*> File Output Format Counters <*> Bytes Written<*><*><*>,2
33f0f472,finish HadoopPrepareTerasort bench<*>,2
47df6e7d,total used free shared buff<*> available<*> <*> <*> <*> <*> <*> <*><*> <*> <*> <*> <*> <*> <*> <*>,16
abdeec3d,Start changing mapper and reducer ...<*>,22
0b112714,<*>Start execute hibench datasize <*> mapper <*> reducer <*> <*><*>,22
a83566c9,Running <*> test ...<*>,27
c49fab9e,USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND<*> <*> <*>.<*> <*>.<*> <*> <*> tty1 Ss<*> <*> <*> <*> <*> <*> <*>noclear tty1 linux<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*> <*> <*> <*> <*> <*>init<*>file <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*> <*> <*> <*> sh run_terasort.sh<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> R <*> <*> <*> <*> ps au<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*> <*> <*> <*> <*> <*>init<*>file <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*> <*> <*> <*> <*> <*>init<*>file <*>,13
f8f81afb,start HadoopTerasort bench<*>,22
4552c30b,hdfs du <*>s <*> <*>config <*> fs <*>du <*>s hdfs <*>,27
baac531a,Submit MapReduce Job <*> <*>config <*> jar <*> terasort <*>D <*> <*>D <*> hdfs <*> hdfs <*>,22
9355e6c6,22<*> <*> <*> <*> INFO terasort.TeraSort <*>,45
32a067c8,22<*> <*> <*> <*> INFO <*> Total input files to process <*><*>,40
13ad3876,Spent <*> computing base<*>splits.<*>,22
ff975c27,Spent <*> computing TeraScheduler splits.<*>,22
dad1f533,Computing input splits took <*>,22
68e224eb,Sampling <*> splits of <*><*>,22
c47ca505,Making <*> from <*> sampled records<*>,22
9c3e190a,Computing parititions took <*>,22
592be439,Spent <*> computing partitions.<*>,22
045a9c40,22<*> <*> <*> <*> INFO mapreduce.Job Counters <*><*> File System Counters<*> FILE Number of bytes read<*><*><*> FILE Number of bytes written<*><*><*> FILE Number of read operations<*><*><*> FILE Number of large read operations<*><*><*> FILE Number of write operations<*><*><*> HDFS Number of bytes read<*><*><*> HDFS Number of bytes written<*><*><*> HDFS Number of read operations<*><*><*> HDFS Number of large read operations<*><*><*> HDFS Number of write operations<*><*><*> Job Counters <*> Launched map tasks<*><*><*> Launched reduce tasks<*><*><*> Other local map tasks<*><*><*> Data<*>local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total time spent by all reduce tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total vcore<*>milliseconds taken by all reduce tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all reduce tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Map output bytes<*><*><*> Map output materialized bytes<*><*><*> Input split bytes<*><*><*> Combine input records<*><*><*> Combine output records<*><*><*> Reduce input groups<*><*><*> Reduce shuffle bytes<*><*><*> Reduce input records<*><*><*> Reduce output records<*><*><*> Spilled Records<*><*><*> Shuffled Maps <*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*> Peak Reduce Physical memory (bytes)<*><*><*> Peak Reduce Virtual memory (bytes)<*><*><*> Shuffle Errors<*> BAD_ID<*><*><*> CONNECTION<*><*><*> IO_ERROR<*><*><*> WRONG_LENGTH<*><*><*> WRONG_MAP<*><*><*> WRONG_REDUCE<*><*><*> File Input Format Counters <*> Bytes Read<*><*><*> File Output Format Counters <*> Bytes Written<*><*><*>,13
ded1e603,finish HadoopTerasort bench<*>,23
c63292d7,top failed tty get<*>,16
fb3b04d3,Filesystem 1K<*>blocks Used Available Use% Mounted on<*> <*> <*> <*> <*>% <*> <*> <*> <*> <*>% <*> <*> <*> <*> <*>% <*> <*> <*> <*> <*>% <*> <*> <*> <*> <*>% <*> <*> <*> <*> <*>% <*> <*> <*> <*> <*>% <*>,16
843f62a3,"USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND<*> <*> <*>.<*> <*>.<*> <*> <*> tty1 Ss<*> <*> <*> <*> <*> <*> <*>noclear tty1 linux<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*> <*> <*> <*> <*> <*>init<*>file <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*> <*> <*> <*> sh run_terasort.sh<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Sl <*> <*> <*> <*> python2 <*> HadoopTerasort <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*> <*> <*> <*> ssh <*> python <*>u <*>c ""exec('\nimport time os sys socket <*> <*> <*> func_<*>() \n with open(\""<*> as f \n <*> for x in f.readlines() if x.startswith(\""cpu\"")]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>() \n with open(\""<*> as f \n <*> for x in f.readlines()]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>() \n with open(\""<*> as f \n blocks <*> os.listdir(\""<*> <*> for x in f.readlines() if x.split()[<*>] in blocks and not <*> and x.split()[<*>]!<*>\""<*>\""]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>() \n with open(\""<*> as f \n mem <*> dict([(a <*> for a b in [x.split(\"" \"") for x in f.readlines()]])\n <*> for field in [\""MemTotal\"" \""Buffers\"" \""Cached\"" \""MemFree\"" \""Mapped\""]])<*>chr(<*>))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>() \n with open(\""<*> as f \n <*> \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\nwhile <*> \n s2.send((\""<*>SEP<*>%s\"" % time.time())<*>chr(<*>))\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n s2.send(\""<*>SEP<*>#end\""<*>chr(<*>))\n <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*> <*> <*> <*> ssh <*> python <*>u <*>c ""exec('\nimport time os sys socket <*> <*> <*> func_<*>() \n with open(\""<*> as f \n <*> for x in f.readlines() if x.startswith(\""cpu\"")]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>() \n with open(\""<*> as f \n <*> for x in f.readlines()]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>() \n with open(\""<*> as f \n blocks <*> os.listdir(\""<*> <*> for x in f.readlines() if x.split()[<*>] in blocks and not <*> and x.split()[<*>]!<*>\""<*>\""]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>() \n with open(\""<*> as f \n mem <*> dict([(a <*> for a b in [x.split(\"" \"") for x in f.readlines()]])\n <*> for field in [\""MemTotal\"" \""Buffers\"" \""Cached\"" \""MemFree\"" \""Mapped\""]])<*>chr(<*>))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>() \n with open(\""<*> as f \n <*> \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\nwhile <*> \n s2.send((\""<*>SEP<*>%s\"" % time.time())<*>chr(<*>))\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n s2.send(\""<*>SEP<*>#end\""<*>chr(<*>))\n <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*> <*> <*> <*> ssh <*> python <*>u <*>c ""exec('\nimport time os sys socket <*> <*> <*> func_<*>() \n with open(\""<*> as f \n <*> for x in f.readlines() if x.startswith(\""cpu\"")]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>() \n with open(\""<*> as f \n <*> for x in f.readlines()]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>() \n with open(\""<*> as f \n blocks <*> os.listdir(\""<*> <*> for x in f.readlines() if x.split()[<*>] in blocks and not <*> and x.split()[<*>]!<*>\""<*>\""]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>() \n with open(\""<*> as f \n mem <*> dict([(a <*> for a b in [x.split(\"" \"") for x in f.readlines()]])\n <*> for field in [\""MemTotal\"" \""Buffers\"" \""Cached\"" \""MemFree\"" \""Mapped\""]])<*>chr(<*>))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>() \n with open(\""<*> as f \n <*> \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\nwhile <*> \n s2.send((\""<*>SEP<*>%s\"" % time.time())<*>chr(<*>))\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n s2.send(\""<*>SEP<*>#end\""<*>chr(<*>))\n <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*> <*> <*> <*> ssh <*> python <*>u <*>c ""exec('\nimport time os sys socket <*> <*> <*> func_<*>() \n with open(\""<*> as f \n <*> for x in f.readlines() if x.startswith(\""cpu\"")]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>() \n with open(\""<*> as f \n <*> for x in f.readlines()]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>() \n with open(\""<*> as f \n blocks <*> os.listdir(\""<*> <*> for x in f.readlines() if x.split()[<*>] in blocks and not <*> and x.split()[<*>]!<*>\""<*>\""]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>() \n with open(\""<*> as f \n mem <*> dict([(a <*> for a b in [x.split(\"" \"") for x in f.readlines()]])\n <*> for field in [\""MemTotal\"" \""Buffers\"" \""Cached\"" \""MemFree\"" \""Mapped\""]])<*>chr(<*>))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>() \n with open(\""<*> as f \n <*> \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\nwhile <*> \n s2.send((\""<*>SEP<*>%s\"" % time.time())<*>chr(<*>))\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n s2.send(\""<*>SEP<*>#end\""<*>chr(<*>))\n <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> R <*> <*> <*> <*> ps au<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*> <*> <*> <*> <*> <*>init<*>file <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*> <*> <*> <*> <*> <*>init<*>file <*>",3
0f2facb4,22<*> <*> <*> <*> INFO mapreduce.Job Counters <*><*> File System Counters<*> FILE Number of bytes read<*><*><*> FILE Number of bytes written<*><*><*> FILE Number of read operations<*><*><*> FILE Number of large read operations<*><*><*> FILE Number of write operations<*><*><*> HDFS Number of bytes read<*><*><*> HDFS Number of bytes written<*><*><*> HDFS Number of read operations<*><*><*> HDFS Number of large read operations<*><*><*> HDFS Number of write operations<*><*><*> Job Counters <*> Launched map tasks<*><*><*> Launched reduce tasks<*><*><*> Data<*>local map tasks<*><*><*> Rack<*>local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total time spent by all reduce tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total vcore<*>milliseconds taken by all reduce tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all reduce tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Map output bytes<*><*><*> Map output materialized bytes<*><*><*> Input split bytes<*><*><*> Combine input records<*><*><*> Combine output records<*><*><*> Reduce input groups<*><*><*> Reduce shuffle bytes<*><*><*> Reduce input records<*><*><*> Reduce output records<*><*><*> Spilled Records<*><*><*> Shuffled Maps <*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*> Peak Reduce Physical memory (bytes)<*><*><*> Peak Reduce Virtual memory (bytes)<*><*><*> Shuffle Errors<*> BAD_ID<*><*><*> CONNECTION<*><*><*> IO_ERROR<*><*><*> WRONG_LENGTH<*><*><*> WRONG_MAP<*><*><*> WRONG_REDUCE<*><*><*> File Input Format Counters <*> Bytes Read<*><*><*> File Output Format Counters <*> Bytes Written<*><*><*>,4
bd7a59e5,PID USER PR NI VIRT RES SHR ST %CPU %MEM TIME<*> COMMAND <*> mqy <*> <*> <*>.3g <*> <*> R <*>.<*> <*>.<*> <*> <*> node <*> wwmm <*> <*> <*>.1g <*>.6g <*>.2g R <*>.<*> <*>.<*> <*> <*>.<*> python <*> wwmm <*> <*> <*>.5g <*>.4g <*>.2g S <*>.<*> <*>.<*> <*> <*>.<*> python <*> root <*> <*> <*> <*> <*> R <*>.<*> <*>.<*> <*> <*>.<*> kworker<*> <*> yyl <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> nvidia<*>smi <*> xjjl <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> node <*> xjjl <*> <*> <*> <*> <*> R <*>.<*> <*>.<*> <*> <*>.<*> top <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> containerd <*> hyy <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> node <*> mysql <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> mysqld <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*> <*>.<*> rcu_sched <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> irq<*> <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*> <*>.<*> kworker<*> <*> <*> hyy <*> <*> <*>.0g <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> java <*>,3
b6fa4823,22<*> <*> <*> <*> INFO mapreduce.Job Counters <*><*> File System Counters<*> FILE Number of bytes read<*><*><*> FILE Number of bytes written<*><*><*> FILE Number of read operations<*><*><*> FILE Number of large read operations<*><*><*> FILE Number of write operations<*><*><*> HDFS Number of bytes read<*><*><*> HDFS Number of bytes written<*><*><*> HDFS Number of read operations<*><*><*> HDFS Number of large read operations<*><*><*> HDFS Number of write operations<*><*><*> Job Counters <*> Launched map tasks<*><*><*> Launched reduce tasks<*><*><*> Other local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total time spent by all reduce tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total vcore<*>milliseconds taken by all reduce tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all reduce tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Map output bytes<*><*><*> Map output materialized bytes<*><*><*> Input split bytes<*><*><*> Combine input records<*><*><*> Combine output records<*><*><*> Reduce input groups<*><*><*> Reduce shuffle bytes<*><*><*> Reduce input records<*><*><*> Reduce output records<*><*><*> Spilled Records<*><*><*> Shuffled Maps <*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*> Peak Reduce Physical memory (bytes)<*><*><*> Peak Reduce Virtual memory (bytes)<*><*><*> Shuffle Errors<*> BAD_ID<*><*><*> CONNECTION<*><*><*> IO_ERROR<*><*><*> WRONG_LENGTH<*><*><*> WRONG_MAP<*><*><*> WRONG_REDUCE<*><*><*> File Input Format Counters <*> Bytes Read<*><*><*> File Output Format Counters <*> Bytes Written<*><*><*>,3
2d7eda49,22<*> <*> <*> <*> INFO <*> <*> <*> <*> <*> <*> <*> Number of <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> reduce <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> tasks <*>,5
82fca437,PID USER PR NI VIRT RES SHR ST %CPU %MEM TIME<*> COMMAND <*> mqy <*> <*> <*>.3g <*> <*> R <*>.<*> <*>.<*> <*> <*> node <*> wwmm <*> <*> <*>.1g <*>.6g <*>.2g R <*>.<*> <*>.<*> <*> <*>.<*> python <*> wwmm <*> <*> <*>.5g <*>.4g <*>.2g S <*>.<*> <*>.<*> <*> <*>.<*> python <*> root <*> <*> <*> <*> <*> R <*>.<*> <*>.<*> <*> <*>.<*> kworker<*> <*> yyl <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> nvidia<*>smi <*> xjjl <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> node <*> xjjl <*> <*> <*> <*> <*> R <*>.<*> <*>.<*> <*> <*>.<*> top <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> containerd <*> hyy <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> node <*> mysql <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> mysqld <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*> <*>.<*> rcu_sched <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> irq<*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*> <*>.<*> kworker<*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*> <*>.<*> kworker<*> <*> hyy <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> sshd <*> jiaqi <*> <*> <*>.3g <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> node <*>,1
f1c84cb5,PID USER PR NI VIRT RES SHR ST %CPU %MEM TIME<*> COMMAND <*> mqy <*> <*> <*>.3g <*> <*> R <*>.<*> <*>.<*> <*> <*> node <*> wwmm <*> <*> <*>.1g <*>.6g <*>.2g R <*>.<*> <*>.<*> <*> <*>.<*> python <*> wwmm <*> <*> <*>.5g <*>.4g <*>.2g S <*>.<*> <*>.<*> <*> <*>.<*> python <*> root <*> <*> <*> <*> <*> R <*>.<*> <*>.<*> <*> <*>.<*> kworker<*> <*> xjjl <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> node <*> xjjl <*> <*> <*> <*> <*> R <*>.<*> <*>.<*> <*> <*>.<*> top <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> containerd <*> hyy <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> node <*> mysql <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> mysqld <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*> <*>.<*> rcu_sched <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> irq<*> <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*> <*>.<*> kworker<*> <*> <*> hyy <*> <*> <*>.0g <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> java <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*> <*>.<*> kworker<*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*> <*>.<*> kworker<*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*> <*>.<*> kworker<*> <*> hyy <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> sshd <*> jiaqi <*> <*> <*>.3g <*> <*> S <*>.<*> <*>.<*> <*> <*>.<*> node <*>,1
01d43520,Type Date Time Input_data_size Duration(s) Throughput(bytes<*> Throughput<*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*> <*> <*>,2002
cfb147f5,start HadoopPrepareBayes bench<*>,1
131f7681,Submit MapReduce Job <*> <*>config <*> jar <*> HiBench.DataGen <*>t <*> <*>b hdfs <*> <*>n Input <*>m <*> <*>r <*> <*>p <*> <*> <*> <*>o <*>,2
9d5e9f5b,22<*> <*> <*> <*> INFO HiBench.Dummy Creating dummy file hdfs <*> with <*> <*>,2
6618ecde,curIndex <*> total <*><*>,2
7a30f4e7,22<*> <*> <*> <*> INFO HiBench.BayesData <*> bayes <*> data <*> <*>,2
8c8c100a,22<*> <*> <*> <*> INFO HiBench.BayesData Running Job Create bayes data<*>,2
6dbe12a6,22<*> <*> <*> <*> INFO <*> <*> file hdfs <*> as <*>,6
e6de700f,22<*> <*> <*> <*> WARN mapreduce.JobResourceUploader Hadoop command<*>line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.<*>,3
313cc193,22<*> <*> <*> <*> INFO mapreduce.Job Counters <*><*> File System Counters<*> FILE Number of bytes read<*><*><*> FILE Number of bytes written<*><*><*> FILE Number of read operations<*><*><*> FILE Number of large read operations<*><*><*> FILE Number of write operations<*><*><*> HDFS Number of bytes read<*><*><*> HDFS Number of bytes written<*><*><*> HDFS Number of read operations<*><*><*> HDFS Number of large read operations<*><*><*> HDFS Number of write operations<*><*><*> Job Counters <*> Launched map tasks<*><*><*> Other local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Input split bytes<*><*><*> Spilled Records<*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*> HiBench.Counters<*> BYTES_DATA_GENERATED<*><*><*> File Input Format Counters <*> Bytes Read<*><*><*> File Output Format Counters <*> Bytes Written<*><*><*>,2
0e37b332,finish HadoopPrepareBayes bench<*>,1
fe1e4651,start HadoopPreparePagerank bench<*>,1
1e05310a,22<*> <*> <*> <*> INFO HiBench.PagerankData <*> <*> data <*>,3
9ff6602e,22<*> <*> <*> <*> INFO HiBench.PagerankData Creating PageRank <*>,2
0426dc53,22<*> <*> <*> <*> INFO HiBench.PagerankData Running Job Create pagerank <*>,4
59621183,finish HadoopPreparePagerank bench<*>,1
d6911c83,start HadoopBayes bench<*>,2
26f683b1,Export env HADOOP_EXECUTABLE<*><*>,2
662bcf94,Export env HADOOP_HOME<*><*>,2
120df2c9,Export env HADOOP_CONF_DIR<*><*>,2
d2dd9328,MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.<*>,5
9ada3831,Running on hadoop using <*> and HADOOP_CONF_DIR<*><*>,5
aeb61de0,MAHOUT<*>JOB <*>,5
7650df9d,22<*> <*> <*> <*> INFO SparseVectorsFromSequenceFiles Maximum n<*>gram size is <*><*>,3
9842dbb8,22<*> <*> <*> <*> INFO SparseVectorsFromSequenceFiles <*>,7
e66ecb0b,22<*> <*> <*> <*> INFO SparseVectorsFromSequenceFiles Tokenizing documents in hdfs <*>,2
e12c62e9,22<*> <*> <*> <*> INFO TokenCache Got dt for hdfs <*> Kind HDFS_DELEGATION_TOKEN Service ha<*>hdfs cuhkcluster Ident (token for team7 HDFS_DELEGATION_TOKEN owner<*>team7@BIGDATA renewer<*>yarn realUser<*> issueDate<*><*> maxDate<*><*> sequenceNumber<*><*> masterKeyId<*><*>)<*>,13
7084d5a6,22<*> <*> <*> <*> INFO deprecation <*> is deprecated. Instead use <*>,44
eaa11026,22<*> <*> <*> <*> INFO Job Job job_<*>_<*> <*>,25
3e67bc08,22<*> <*> <*> <*> INFO Job Counters <*><*> File System Counters<*> FILE Number of bytes read<*><*><*> FILE Number of bytes written<*><*><*> FILE Number of read operations<*><*><*> FILE Number of large read operations<*><*><*> FILE Number of write operations<*><*><*> HDFS Number of bytes read<*><*><*> HDFS Number of bytes written<*><*><*> HDFS Number of read operations<*><*><*> HDFS Number of large read operations<*><*><*> HDFS Number of write operations<*><*><*> Job Counters <*> Launched map tasks<*><*><*> Data<*>local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Input split bytes<*><*><*> Spilled Records<*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*> File Input Format Counters <*> Bytes Read<*><*><*> File Output Format Counters <*> Bytes Written<*><*><*>,13
2740cedf,22<*> <*> <*> <*> INFO SparseVectorsFromSequenceFiles Creating Term Frequency Vectors<*>,2
5b3ba9c0,22<*> <*> <*> <*> INFO DictionaryVectorizer Creating dictionary from hdfs <*> and saving at hdfs <*>,3
53f12e2f,mysql> select * from mysql.general_log;<*> event_time <*> user_host <*> thread_id <*> server_id <*> command_type <*> argument <*><*> <*> <*> <*> <*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Query <*> show variables like 'general%' <*><*> <*> <*> <*> <*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Query <*> show variables like 'log_output' <*><*> <*> <*> <*> <*> <*> [root] @ localhost [] <*> <*> <*> <*> <*> Connect <*> root@localhost on <*><*> <*> <*> <*> <*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Query <*> KILL QUERY <*> <*><*> <*> <*> <*> <*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Quit <*> <*><*> <*> <*> <*> <*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Query <*> select * from mysql.general_log <*><*>,1
c862d95d,22<*> <*> <*> <*> INFO Job Task Id attempt_<*>_<*>_r_<*>_<*> Status FAILED<*> <*> <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> by <*> File does not exist hdfs <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> ... <*> more<*>,4
1c019e0f,[<*> <*> <*> <*>.<*>]File does not exist hdfs <*> File does not exist hdfs <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*>,1
1cc7f185,Job failed as tasks failed. failedMaps <*> failedReduces <*> killedMaps <*> killedReduces <*><*>,3
f75bb5e8,"22<*> <*> <*> <*> INFO Job Counters <*><*> File System Counters<*> FILE Number of bytes read<*><*><*> FILE Number of bytes written<*><*><*> FILE Number of read operations<*><*><*> FILE Number of large read operations<*><*><*> FILE Number of write operations<*><*><*> HDFS Number of bytes read<*><*><*> HDFS Number of bytes written<*><*><*> HDFS Number of read operations<*><*><*> HDFS Number of large read operations<*><*><*> HDFS Number of write operations<*><*><*> Job Counters <*> Failed reduce tasks<*><*><*> Launched map tasks<*><*><*> Launched reduce tasks<*><*><*> Data<*>local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total time spent by all reduce tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total vcore<*>milliseconds taken by all reduce tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all reduce tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Map output bytes<*><*><*> Map output materialized bytes<*><*><*> Input split bytes<*><*><*> Combine input records<*><*><*> Spilled Records<*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*> File Input Format Counters <*> Bytes Read<*><*><*> in thread ""main"" <*> Job failed!<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*>",2
102fdb37,22<*> <*> <*> <*> WARN MahoutDriver No trainnb.props found on classpath will use command<*>line arguments only<*>,2
fe55ce57,22<*> <*> <*> <*> INFO AbstractJob Command line arguments {<*>alphaI<*>[<*>.<*>] <*>endPhase<*>[<*>] <*>input<*>[hdfs <*> <*>labelIndex<*>[hdfs <*> <*>output<*>[hdfs <*> <*>overwrite<*>null <*>startPhase<*>[<*>] <*>tempDir<*>[hdfs <*>,2
99a20a7c,"22<*> <*> <*> <*> INFO JobSubmitter Cleaning up the staging area <*> in thread ""main"" <*> Input path does not exist hdfs <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*>",2
50922c3d,finish HadoopBayes bench<*>,4
0c3feb91,start HadoopPagerank bench<*>,3
9e3059e3,rm `hdfs <*> No such file or directory<*>,1
05351b24,Submit MapReduce Job <*> <*>config <*> jar <*> pegasus.PagerankNaive hdfs <*> hdfs <*> <*> <*> <*> nosym new<*>,3
159168d9,<*>[PEGASUS A Peta<*>Scale Graph Mining System]<*><*>,3
db16a6e0,[PEGASUS] Computing PageRank. Max iteration <*> <*> threshold <*> <*>.<*> cur_iteration<*><*><*>,3
2f474ae5,Creating initial pagerank <*>,3
86098fce,mysql> select * from mysql.general_log;<*> event_time <*> user_host <*> thread_id <*> server_id <*> command_type <*> argument <*><*> <*> <*> <*> <*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Query <*> select * from MyDB.test <*><*> <*> <*> <*> <*> <*> [root] @ localhost [] <*> <*> <*> <*> <*> Connect <*> root@localhost on <*><*> <*> <*> <*> <*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Query <*> KILL QUERY <*> <*><*> <*> <*> <*> <*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Quit <*> <*><*> <*> <*> <*> <*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Query <*> select * from mysql.general_log <*><*>,1
bba81176,22<*> <*> <*> <*> INFO MahoutDriver Program took <*> ms (Minutes <*>.<*>)<*>,2
f3bed279,22<*> <*> <*> <*> INFO Job Task Id attempt_<*>_<*>_r_<*>_<*> Status FAILED<*> <*> File does not exist <*> (inode <*>) Holder DFSClient_attempt_<*>_<*>_r_<*>_<*>_<*>_<*> does not have any open files.<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> by <*> File does not exist <*> (inode <*>) Holder DFSClient_attempt_<*>_<*>_r_<*>_<*>_<*>_<*> does not have any open files.<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Source)<*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Source)<*> at <*> ... <*> more<*>,1
1af99126,"Traceback (most recent call last) <*> File ""<stdin>"" line <*> in <module><*> list index out of range<*>",1
fb0c81c3,Iteration <*> <*> changed reducer <*> <*><*>,1
3fae6721,[PEGASUS] PageRank computed.<*>,1
b6b6e164,[PEGASUS] The final PageRanks are in the HDFS pr_vector.<*>,1
ee9952cd,finish HadoopPagerank bench<*>,1
a7e386bb,22<*> <*> <*> <*> INFO HadoopUtil Deleting hdfs <*>,1
7afd5efb,22<*> <*> <*> <*> INFO Job Task Id attempt_<*>_<*>_m_<*>_<*> Status FAILED<*> <*> File does not exist hdfs <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*>,9
9dc3a910,"22<*> <*> <*> <*> INFO Job Counters <*><*> Job Counters <*> Failed map tasks<*><*><*> Killed map tasks<*><*><*> Killed reduce tasks<*><*><*> Launched map tasks<*><*><*> Other local map tasks<*><*><*> Data<*>local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Map<*>Reduce Framework<*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> in thread ""main"" <*> Job failed!<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*>",1
18ffca22,"Traceback (most recent call last) <*> File ""<stdin>"" line <*> in <module><*> No module named 'nonexistmodule' .<*>",1
e2aab0b3,"Traceback (most recent call last) <*> File ""<stdin>"" line <*> in <module><*> File ""<stdin>"" line <*> in demo<*> File ""<stdin>"" line <*> in demo<*> File ""<stdin>"" line <*> in demo<*> [Previous line repeated <*> more times]<*> File ""<stdin>"" line <*> in demo<*> maximum recursion depth exceeded in comparison<*>",1
