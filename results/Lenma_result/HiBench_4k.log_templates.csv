EventId,EventTemplate,Occurrences
334dad93,nohup: ignoring input<*>,3
fef2f1a8,<*> Start obtain and cache Kerberos ticket<*>granting ticket <*><*>,3
b1154f51,Password for team7@BIGDATA: <*>,3
f7302a4b,<*> Start execute hibench test <*><*>,3
be5a0cc3,mkdir: cannot create directory  File exists<*>,34
26bef9d4,Start changing datasize ...<*>,3
e2f1a978,Creating  data ...<*>,5
639333f4,patching args<*><*>,32
ca468b23,Parsing conf: <*>,128
53c74c86,probe sleep jar: <*>,32
1f267189,"ERROR, execute cmd: '( <*> node <*>list <*>> <*> <*> grep RUNNING )' timedout.<*> STDOU<*> STDER<*> Please check!<*>",1
52444156,"Traceback (most recent call last):<*> File ""<*> line <*>, in <module><*> load_config(conf_root, workload_configFile, workload_folder, patching_config)<*> File ""<*> line <*>, in load_config<*> generate_optional_value()<*> File ""<*> line <*>, in generate_optional_value<*> probe_masters_slaves_hostnames()<*> File ""<*> line <*>, in probe_masters_slaves_hostnames<*> probe_masters_slaves_by_Yarn()<*> File ""<*> line <*>, in probe_masters_slaves_by_Yarn<*> assert <*>, ""Get workers from yarn<*>site.xml page failed, reason:%s\nplease set <*> and <*> manually"" % e<*> Get workers from yarn<*>site.xml page failed, reason:( <*> node <*>list <*>> <*> <*> grep RUNNING ) executed timedout for <*> seconds<*>",1
fb7d48a7,please set <*> and <*> manually<*>,1
9997a02c,<*> line <*>: .: filename argument required<*>,1
805c90d8,start HadoopPrepareTerasort bench<*>,2
537c8444,hdfs rm <*>r: <*> <*>config <*> fs <*>rm <*>r <*>skipTrash hdfs:<*>,31
76300dd5,Deleted hdfs:<*>,30
749c8a55,Submit MapReduce Job: <*> <*>config <*> jar <*> teragen <*>D <*> <*>D <*> <*>D <*> <*> hdfs:<*>,2
3d3e58ef,22<*> <*>:<*>:<*> INFO client.AHSProxy: Connecting to Application History server at <*>,38
71c00861,"22<*> <*>:<*>:<*> INFO  Created token for team7: HDFS_DELEGATION_TOKEN owner<*>team7@BIGDATA, renewer<*>yarn, realUser<*>, issueDate<*><*>, maxDate<*><*>, sequenceNumber<*><*>, masterKeyId<*><*> on ha<*>hdfs:cuhkcluster<*>",44
74d0fe80,22<*> <*>:<*>:<*> INFO       ,138
04af8f52,"22<*> <*>:<*>:<*> INFO  New token received: (Kind: kms<*>dt, Service: <*> Ident: (kms<*>dt owner<*>team7, renewer<*>yarn, realUser<*>, issueDate<*><*>, maxDate<*><*>, sequenceNumber<*><*>, masterKeyId<*><*>))<*>",44
7741af87,"22<*> <*>:<*>:<*> INFO  Got dt for hdfs:<*> Kind: HDFS_DELEGATION_TOKEN, Service: ha<*>hdfs:cuhkcluster, Ident: (token for team7: HDFS_DELEGATION_TOKEN owner<*>team7@BIGDATA, renewer<*>yarn, realUser<*>, issueDate<*><*>, maxDate<*><*>, sequenceNumber<*><*>, masterKeyId<*><*>)<*>",44
a0b5dc60,"22<*> <*>:<*>:<*> INFO  Got dt for hdfs:<*> Kind: kms<*>dt, Service: <*> Ident: (kms<*>dt owner<*>team7, renewer<*>yarn, realUser<*>, issueDate<*><*>, maxDate<*><*>, sequenceNumber<*><*>, masterKeyId<*><*>)<*>",44
935ad869,22<*> <*>:<*>:<*> INFO     ,357
e7f606ed,22<*> <*>:<*>:<*> INFO    ,129
1201dca5,22<*> <*>:<*>:<*> INFO      ,68
bd058b1d,"22<*> <*>:<*>:<*> INFO  Executing with tokens: [Kind: HDFS_DELEGATION_TOKEN, Service: ha<*>hdfs:cuhkcluster, Ident: (token for team7: HDFS_DELEGATION_TOKEN owner<*>team7@BIGDATA, renewer<*>yarn, realUser<*>, issueDate<*><*>, maxDate<*><*>, sequenceNumber<*><*>, masterKeyId<*><*>), Kind: kms<*>dt, Service: <*> Ident: (kms<*>dt owner<*>team7, renewer<*>yarn, realUser<*>, issueDate<*><*>, maxDate<*><*>, sequenceNumber<*><*>, masterKeyId<*><*>)]<*>",42
6418ab1e,22<*> <*>:<*>:<*> INFO conf.Configuration: found resource resource<*>types.xml at file:<*>,21
f8987a3d,22<*> <*>:<*>:<*> INFO        ,98
8f5d9d1e,22<*> <*>:<*>:<*> INFO  Job job_<*>_<*> running in uber mode : <*><*>,43
3394da8c,22<*> <*>:<*>:<*> INFO mapreduce.Job: Counters: <*><*> File System Counters<*> FILE: Number of bytes read<*><*><*> FILE: Number of bytes written<*><*><*> FILE: Number of read operations<*><*><*> FILE: Number of large read operations<*><*><*> FILE: Number of write operations<*><*><*> HDFS: Number of bytes read<*><*><*> HDFS: Number of bytes written<*><*><*> HDFS: Number of read operations<*><*><*> HDFS: Number of large read operations<*><*><*> HDFS: Number of write operations<*><*><*> Job Counters <*> Launched map tasks<*><*><*> Other local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Input split bytes<*><*><*> Spilled Records<*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*>   File Input Format Counters <*> Bytes Read<*><*><*> File Output Format Counters <*> Bytes Written<*><*><*>,4
e4ba8c7f,finish HadoopPrepareTerasort bench<*>,2
da9a00a1,total used free shared buff<*> available<*> <*> <*> <*> <*> <*> <*><*> <*> <*> <*> <*> <*> <*> <*>,1
d3310469,Start changing mapper and reducer ...<*>,22
4793a9ca,"<*>Start execute hibench datasize:  mapper: <*>, reducer: <*> <*><*>",22
60e97823,Running  test ...<*>,27
0299bdbe,USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND<*> <*> <*>.<*> <*>.<*> <*> <*> tty1 Ss<*> <*>:<*> <*>:<*> <*> <*>noclear tty1 linux<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> sh run_terasort.sh<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> R <*>:<*> <*>:<*> ps au<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*>,12
119ff0ee,start HadoopTerasort bench<*>,22
a649956a,hdfs du <*>s: <*> <*>config <*> fs <*>du <*>s hdfs:<*>,27
420c1033,Submit MapReduce Job: <*> <*>config <*> jar <*> terasort <*>D <*> <*>D <*> hdfs:<*> hdfs:<*>,22
f7983f50,22<*> <*>:<*>:<*> INFO  ,46
b01d763a,Spent  computing base<*>splits.<*>,22
95bd1c69,Spent  computing TeraScheduler splits.<*>,22
876330b8,Computing input splits took ,22
2c13861e,Sampling <*> splits of <*><*>,22
09e40cba,Making <*> from <*> sampled records<*>,22
152a62c7,Computing parititions took ,22
fafca6f8,Spent  computing partitions.<*>,22
5dd2ed92,22<*> <*>:<*>:<*> INFO mapreduce.Job: Counters: <*><*> File System Counters<*> FILE: Number of bytes read<*><*><*> FILE: Number of bytes written<*><*><*> FILE: Number of read operations<*><*><*> FILE: Number of large read operations<*><*><*> FILE: Number of write operations<*><*><*> HDFS: Number of bytes read<*><*><*> HDFS: Number of bytes written<*><*><*> HDFS: Number of read operations<*><*><*> HDFS: Number of large read operations<*><*><*> HDFS: Number of write operations<*><*><*> Job Counters <*> Launched map tasks<*><*><*> Launched reduce tasks<*><*><*> Other local map tasks<*><*><*> Data<*>local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total time spent by all reduce tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total vcore<*>milliseconds taken by all reduce tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all reduce tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Map output bytes<*><*><*> Map output materialized bytes<*><*><*> Input split bytes<*><*><*> Combine input records<*><*><*> Combine output records<*><*><*> Reduce input groups<*><*><*> Reduce shuffle bytes<*><*><*> Reduce input records<*><*><*> Reduce output records<*><*><*> Spilled Records<*><*><*> Shuffled Maps <*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*> Peak Reduce Physical memory (bytes)<*><*><*> Peak Reduce Virtual memory (bytes)<*><*><*> Shuffle Errors<*> BAD_ID<*><*><*> CONNECTION<*><*><*> IO_ERROR<*><*><*> WRONG_LENGTH<*><*><*> WRONG_MAP<*><*><*> WRONG_REDUCE<*><*><*> File Input Format Counters <*> Bytes Read<*><*><*> File Output Format Counters <*> Bytes Written<*><*><*>,13
af97fcfd,finish HadoopTerasort bench<*>,23
ee89bf56,top: failed tty get<*>,16
acdbddb6,Filesystem 1K<*>blocks Used Available Use% Mounted on<*> <*> <*> <*> <*>% <*> <*> <*> <*> <*>% <*> <*> <*> <*> <*>% <*> <*> <*> <*> <*>% <*> <*> <*> <*> <*>% <*> <*> <*> <*> <*>% <*> <*> <*> <*> <*>% <*>,16
85f8e4bb,total used free shared buff<*> available<*> <*> <*> <*> <*> <*> <*><*> <*> <*> <*> <*> <*> <*><*>,15
b1c66ddd,"USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND<*> <*> <*>.<*> <*>.<*> <*> <*> tty1 Ss<*> <*>:<*> <*>:<*> <*> <*>noclear tty1 linux<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> sh run_terasort.sh<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Sl <*>:<*> <*>:<*> python2 <*> HadoopTerasort <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> ssh <*> python <*>u <*>c ""exec('\nimport time, os, sys, socket, <*> <*> <*> func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines() if x.startswith(\""cpu\"")]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines()]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n blocks <*> os.listdir(\""<*> <*> for x in f.readlines() if x.split()[<*>] in blocks and not <*> and x.split()[<*>]!<*>\""<*>\""]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n mem <*> dict([(a, <*> for a, b in [x.split(\"":\"") for x in f.readlines()]])\n <*> for field in [\""MemTotal\"", \""Buffers\"", \""Cached\"", \""MemFree\"", \""Mapped\""]])<*>chr(<*>))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\nwhile <*>:\n s2.send((\""<*>SEP<*>%s\"" % time.time())<*>chr(<*>))\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n s2.send(\""<*>SEP<*>#end\""<*>chr(<*>))\n <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> ssh <*> python <*>u <*>c ""exec('\nimport time, os, sys, socket, <*> <*> <*> func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines() if x.startswith(\""cpu\"")]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines()]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n blocks <*> os.listdir(\""<*> <*> for x in f.readlines() if x.split()[<*>] in blocks and not <*> and x.split()[<*>]!<*>\""<*>\""]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n mem <*> dict([(a, <*> for a, b in [x.split(\"":\"") for x in f.readlines()]])\n <*> for field in [\""MemTotal\"", \""Buffers\"", \""Cached\"", \""MemFree\"", \""Mapped\""]])<*>chr(<*>))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\nwhile <*>:\n s2.send((\""<*>SEP<*>%s\"" % time.time())<*>chr(<*>))\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n s2.send(\""<*>SEP<*>#end\""<*>chr(<*>))\n <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> ssh <*> python <*>u <*>c ""exec('\nimport time, os, sys, socket, <*> <*> <*> func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines() if x.startswith(\""cpu\"")]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines()]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n blocks <*> os.listdir(\""<*> <*> for x in f.readlines() if x.split()[<*>] in blocks and not <*> and x.split()[<*>]!<*>\""<*>\""]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n mem <*> dict([(a, <*> for a, b in [x.split(\"":\"") for x in f.readlines()]])\n <*> for field in [\""MemTotal\"", \""Buffers\"", \""Cached\"", \""MemFree\"", \""Mapped\""]])<*>chr(<*>))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\nwhile <*>:\n s2.send((\""<*>SEP<*>%s\"" % time.time())<*>chr(<*>))\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n s2.send(\""<*>SEP<*>#end\""<*>chr(<*>))\n <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> ssh <*> python <*>u <*>c ""exec('\nimport time, os, sys, socket, <*> <*> <*> func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines() if x.startswith(\""cpu\"")]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines()]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n blocks <*> os.listdir(\""<*> <*> for x in f.readlines() if x.split()[<*>] in blocks and not <*> and x.split()[<*>]!<*>\""<*>\""]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n mem <*> dict([(a, <*> for a, b in [x.split(\"":\"") for x in f.readlines()]])\n <*> for field in [\""MemTotal\"", \""Buffers\"", \""Cached\"", \""MemFree\"", \""Mapped\""]])<*>chr(<*>))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\nwhile <*>:\n s2.send((\""<*>SEP<*>%s\"" % time.time())<*>chr(<*>))\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n s2.send(\""<*>SEP<*>#end\""<*>chr(<*>))\n <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> R <*>:<*> <*>:<*> ps au<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*>",2
3c901a9d,22<*> <*>:<*>:<*> INFO mapreduce.Job: Counters: <*><*> File System Counters<*> FILE: Number of bytes read<*><*><*> FILE: Number of bytes written<*><*><*> FILE: Number of read operations<*><*><*> FILE: Number of large read operations<*><*><*> FILE: Number of write operations<*><*><*> HDFS: Number of bytes read<*><*><*> HDFS: Number of bytes written<*><*><*> HDFS: Number of read operations<*><*><*> HDFS: Number of large read operations<*><*><*> HDFS: Number of write operations<*><*><*> Job Counters <*> Launched map tasks<*><*><*> Launched reduce tasks<*><*><*> Data<*>local map tasks<*><*><*> Rack<*>local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total time spent by all reduce tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total vcore<*>milliseconds taken by all reduce tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all reduce tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Map output bytes<*><*><*> Map output materialized bytes<*><*><*> Input split bytes<*><*><*> Combine input records<*><*><*> Combine output records<*><*><*> Reduce input groups<*><*><*> Reduce shuffle bytes<*><*><*> Reduce input records<*><*><*> Reduce output records<*><*><*> Spilled Records<*><*><*> Shuffled Maps <*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*> Peak Reduce Physical memory (bytes)<*><*><*> Peak Reduce Virtual memory (bytes)<*><*><*> Shuffle Errors<*> BAD_ID<*><*><*> CONNECTION<*><*><*> IO_ERROR<*><*><*> WRONG_LENGTH<*><*><*> WRONG_MAP<*><*><*> WRONG_REDUCE<*><*><*> File Input Format Counters <*> Bytes Read<*><*><*> File Output Format Counters <*> Bytes Written<*><*><*>,4
4085fbb9,USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND<*> <*> <*>.<*> <*>.<*> <*> <*> tty1 Ss<*> <*>:<*> <*>:<*> <*> <*>noclear tty1 linux<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> sh run_terasort.sh<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> R <*>:<*> <*>:<*> python2 <*> HadoopTerasort <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> R <*>:<*> <*>:<*> ps au<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*>,1
e74cc0d9,PID USER PR NI VIRT RES SHR ST %CPU %MEM TIME<*> COMMAND <*> mqy <*> <*> <*>.3g <*> <*> R <*>.<*> <*>.<*> <*>:<*> node <*> wwmm <*> <*> <*>.1g <*>.6g <*>.2g R <*>.<*> <*>.<*> <*>:<*>.<*> python <*> wwmm <*> <*> <*>.5g <*>.4g <*>.2g S <*>.<*> <*>.<*> <*>:<*>.<*> python <*> root <*> <*> <*> <*> <*> R <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> yyl <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> nvidia<*>smi <*> xjjl <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> xjjl <*> <*> <*> <*> <*> R <*>.<*> <*>.<*> <*>:<*>.<*> top <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> containerd <*> hyy <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> mysql <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> mysqld <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> rcu_sched <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> irq<*> <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> <*> hyy <*> <*> <*>.0g <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> java <*>,1
5d651cd6,"USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND<*> <*> <*>.<*> <*>.<*> <*> <*> tty1 Ss<*> <*>:<*> <*>:<*> <*> <*>noclear tty1 linux<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> sh run_terasort.sh<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Sl <*>:<*> <*>:<*> python2 <*> HadoopTerasort <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> ssh <*> python <*>u <*>c ""exec('\nimport time, os, sys, socket, <*> <*> <*> func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines() if x.startswith(\""cpu\"")]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines()]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n blocks <*> os.listdir(\""<*> <*> for x in f.readlines() if x.split()[<*>] in blocks and not <*> and x.split()[<*>]!<*>\""<*>\""]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n mem <*> dict([(a, <*> for a, b in [x.split(\"":\"") for x in f.readlines()]])\n <*> for field in [\""MemTotal\"", \""Buffers\"", \""Cached\"", \""MemFree\"", \""Mapped\""]])<*>chr(<*>))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\nwhile <*>:\n s2.send((\""<*>SEP<*>%s\"" % time.time())<*>chr(<*>))\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n s2.send(\""<*>SEP<*>#end\""<*>chr(<*>))\n <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> ssh <*> python <*>u <*>c ""exec('\nimport time, os, sys, socket, <*> <*> <*> func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines() if x.startswith(\""cpu\"")]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines()]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n blocks <*> os.listdir(\""<*> <*> for x in f.readlines() if x.split()[<*>] in blocks and not <*> and x.split()[<*>]!<*>\""<*>\""]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n mem <*> dict([(a, <*> for a, b in [x.split(\"":\"") for x in f.readlines()]])\n <*> for field in [\""MemTotal\"", \""Buffers\"", \""Cached\"", \""MemFree\"", \""Mapped\""]])<*>chr(<*>))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\nwhile <*>:\n s2.send((\""<*>SEP<*>%s\"" % time.time())<*>chr(<*>))\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n s2.send(\""<*>SEP<*>#end\""<*>chr(<*>))\n <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> ssh <*> python <*>u <*>c ""exec('\nimport time, os, sys, socket, <*> <*> <*> func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines() if x.startswith(\""cpu\"")]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines()]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n blocks <*> os.listdir(\""<*> <*> for x in f.readlines() if x.split()[<*>] in blocks and not <*> and x.split()[<*>]!<*>\""<*>\""]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n mem <*> dict([(a, <*> for a, b in [x.split(\"":\"") for x in f.readlines()]])\n <*> for field in [\""MemTotal\"", \""Buffers\"", \""Cached\"", \""MemFree\"", \""Mapped\""]])<*>chr(<*>))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\nwhile <*>:\n s2.send((\""<*>SEP<*>%s\"" % time.time())<*>chr(<*>))\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n s2.send(\""<*>SEP<*>#end\""<*>chr(<*>))\n <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> ssh <*> python <*>u <*>c ""exec('\nimport time, os, sys, socket, <*> <*> <*> func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines() if x.startswith(\""cpu\"")]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines()]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n blocks <*> os.listdir(\""<*> <*> for x in f.readlines() if x.split()[<*>] in blocks and not <*> and x.split()[<*>]!<*>\""<*>\""]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n mem <*> dict([(a, <*> for a, b in [x.split(\"":\"") for x in f.readlines()]])\n <*> for field in [\""MemTotal\"", \""Buffers\"", \""Cached\"", \""MemFree\"", \""Mapped\""]])<*>chr(<*>))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\nwhile <*>:\n s2.send((\""<*>SEP<*>%s\"" % time.time())<*>chr(<*>))\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n s2.send(\""<*>SEP<*>#end\""<*>chr(<*>))\n <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> R <*>:<*> <*>:<*> ps au<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*>",1
0600bb3e,PID USER PR NI VIRT RES SHR ST %CPU %MEM TIME<*> COMMAND <*> mqy <*> <*> <*>.3g <*> <*> R <*>.<*> <*>.<*> <*>:<*> node <*> wwmm <*> <*> <*>.1g <*>.6g <*>.2g R <*>.<*> <*>.<*> <*>:<*>.<*> python <*> wwmm <*> <*> <*>.5g <*>.4g <*>.2g S <*>.<*> <*>.<*> <*>:<*>.<*> python <*> root <*> <*> <*> <*> <*> R <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> yyl <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> nvidia<*>smi <*> xjjl <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> xjjl <*> <*> <*> <*> <*> R <*>.<*> <*>.<*> <*>:<*>.<*> top <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> containerd <*> hyy <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> mysql <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> mysqld <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> rcu_sched <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> irq<*> <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> <*> hyy <*> <*> <*>.0g <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> java <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> hyy <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> sshd <*> jiaqi <*> <*> <*>.3g <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*>,1
e755e052,22<*> <*>:<*>:<*> INFO mapreduce.Job: Counters: <*><*> File System Counters<*> FILE: Number of bytes read<*><*><*> FILE: Number of bytes written<*><*><*> FILE: Number of read operations<*><*><*> FILE: Number of large read operations<*><*><*> FILE: Number of write operations<*><*><*> HDFS: Number of bytes read<*><*><*> HDFS: Number of bytes written<*><*><*> HDFS: Number of read operations<*><*><*> HDFS: Number of large read operations<*><*><*> HDFS: Number of write operations<*><*><*> Job Counters <*> Launched map tasks<*><*><*> Launched reduce tasks<*><*><*> Other local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total time spent by all reduce tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total vcore<*>milliseconds taken by all reduce tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all reduce tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Map output bytes<*><*><*> Map output materialized bytes<*><*><*> Input split bytes<*><*><*> Combine input records<*><*><*> Combine output records<*><*><*> Reduce input groups<*><*><*> Reduce shuffle bytes<*><*><*> Reduce input records<*><*><*> Reduce output records<*><*><*> Spilled Records<*><*><*> Shuffled Maps <*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*> Peak Reduce Physical memory (bytes)<*><*><*> Peak Reduce Virtual memory (bytes)<*><*><*> Shuffle Errors<*> BAD_ID<*><*><*> CONNECTION<*><*><*> IO_ERROR<*><*><*> WRONG_LENGTH<*><*><*> WRONG_MAP<*><*><*> WRONG_REDUCE<*><*><*> File Input Format Counters <*> Bytes Read<*><*><*> File Output Format Counters <*> Bytes Written<*><*><*>,3
c6004681,PID USER PR NI VIRT RES SHR ST %CPU %MEM TIME<*> COMMAND <*> mqy <*> <*> <*>.3g <*> <*> R <*>.<*> <*>.<*> <*>:<*> node <*> wwmm <*> <*> <*>.1g <*>.6g <*>.2g R <*>.<*> <*>.<*> <*>:<*>.<*> python <*> wwmm <*> <*> <*>.5g <*>.4g <*>.2g S <*>.<*> <*>.<*> <*>:<*>.<*> python <*>  <*> <*> <*> <*> <*>  <*>.<*> <*>.<*> <*>:<*>.<*>  <*> xjjl <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> xjjl <*> <*> <*> <*> <*> R <*>.<*> <*>.<*> <*>:<*>.<*> top <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> containerd <*> hyy <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> mysql <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> mysqld <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> rcu_sched <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> irq<*> <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> <*> hyy <*> <*> <*>.0g <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> java <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> hyy <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> sshd <*> jiaqi <*> <*> <*>.3g <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*>,2
58258e3e,22<*> <*>:<*>:<*> INFO  Counters: <*><*> File System Counters<*> FILE: Number of bytes read<*><*><*> FILE: Number of bytes written<*><*><*> FILE: Number of read operations<*><*><*> FILE: Number of large read operations<*><*><*> FILE: Number of write operations<*><*><*> HDFS: Number of bytes read<*><*><*> HDFS: Number of bytes written<*><*><*> HDFS: Number of read operations<*><*><*> HDFS: Number of large read operations<*><*><*> HDFS: Number of write operations<*><*><*> Job Counters <*> Launched map tasks<*><*><*> Launched reduce tasks<*><*><*> Data<*>local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total time spent by all reduce tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total vcore<*>milliseconds taken by all reduce tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all reduce tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Map output bytes<*><*><*> Map output materialized bytes<*><*><*> Input split bytes<*><*><*> Combine input records<*><*><*> Combine output records<*><*><*> Reduce input groups<*><*><*> Reduce shuffle bytes<*><*><*> Reduce input records<*><*><*> Reduce output records<*><*><*> Spilled Records<*><*><*> Shuffled Maps <*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*> Peak Reduce Physical memory (bytes)<*><*><*> Peak Reduce Virtual memory (bytes)<*><*><*> Shuffle Errors<*> BAD_ID<*><*><*> CONNECTION<*><*><*> IO_ERROR<*><*><*> WRONG_LENGTH<*><*><*> WRONG_MAP<*><*><*> WRONG_REDUCE<*><*><*> File Input Format Counters <*> Bytes Read<*><*><*> File Output Format Counters <*> Bytes Written<*><*><*>,11
886eacbd,PID USER PR NI VIRT RES SHR ST %CPU %MEM TIME<*> COMMAND <*> mqy <*> <*> <*>.3g <*> <*> R <*>.<*> <*>.<*> <*>:<*> node <*> wwmm <*> <*> <*>.1g <*>.6g <*>.2g R <*>.<*> <*>.<*> <*>:<*>.<*> python <*> wwmm <*> <*> <*>.5g <*>.4g <*>.2g S <*>.<*> <*>.<*> <*>:<*>.<*> python <*> root <*> <*> <*> <*> <*> R <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> yyl <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> nvidia<*>smi <*> xjjl <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> xjjl <*> <*> <*> <*> <*> R <*>.<*> <*>.<*> <*>:<*>.<*> top <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> containerd <*> hyy <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> mysql <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> mysqld <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> rcu_sched <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> irq<*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> hyy <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> sshd <*> jiaqi <*> <*> <*>.3g <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*>,1
ddf08c0f,Type Date Time Input_data_size Duration(s) Throughput(bytes<*> Throughput<*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*>,2
3195ac97,start HadoopPrepareBayes bench<*>,1
97790b6c,Submit MapReduce Job: <*> <*>config <*> jar <*> HiBench.DataGen <*>t  <*>b hdfs:<*> <*>n Input <*>m <*> <*>r <*> <*>p <*>   <*>o ,2
59a49d8a,"curIndex: <*>, total: <*><*>",2
d1aa9255,22<*> <*>:<*>:<*> WARN mapreduce.JobResourceUploader: Hadoop command<*>line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.<*>,3
9d8378aa,finish HadoopPrepareBayes bench<*>,1
36c030e8,start HadoopPreparePagerank bench<*>,1
cc0b0c78,finish HadoopPreparePagerank bench<*>,1
1b180220,start HadoopBayes bench<*>,2
cfe6bd3a,Export env: HADOOP_EXECUTABLE<*><*>,2
81b0492c,Export env: HADOOP_HOME<*><*>,2
cc2584bd,Export env: HADOOP_CONF_DIR<*><*>,2
91ebd0c4,MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.<*>,5
40635d0e,"Running on hadoop, using <*> and HADOOP_CONF_DIR<*><*>",5
cc2dbc7a,MAHOUT<*>JOB: <*>,5
7c7e1027,22<*> <*>:<*>:<*> INFO Job: Counters: <*><*> File System Counters<*> FILE: Number of bytes read<*><*><*> FILE: Number of bytes written<*><*><*> FILE: Number of read operations<*><*><*> FILE: Number of large read operations<*><*><*> FILE: Number of write operations<*><*><*> HDFS: Number of bytes read<*><*><*> HDFS: Number of bytes written<*><*><*> HDFS: Number of read operations<*><*><*> HDFS: Number of large read operations<*><*><*> HDFS: Number of write operations<*><*><*> Job Counters <*> Launched map tasks<*><*><*> Data<*>local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Input split bytes<*><*><*> Spilled Records<*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*> File Input Format Counters <*> Bytes Read<*><*><*> File Output Format Counters <*> Bytes Written<*><*><*>,3
20daa56f,22<*> <*>:<*>:<*> INFO DictionaryVectorizer: Creating dictionary from hdfs:<*> and saving at hdfs:<*>,3
989fd283,mysql> select * from mysql.general_log;<*> event_time <*> user_host <*> thread_id <*> server_id <*> command_type <*> argument <*><*> <*> <*>:<*>:<*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Query <*> show variables like 'general%' <*><*> <*> <*>:<*>:<*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Query <*> show variables like 'log_output' <*><*> <*> <*>:<*>:<*> <*> [root] @ localhost [] <*> <*> <*> <*> <*> Connect <*> root@localhost on <*><*> <*> <*>:<*>:<*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Query <*> KILL QUERY <*> <*><*> <*> <*>:<*>:<*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Quit <*> <*><*> <*> <*>:<*>:<*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Query <*> select * from mysql.general_log <*><*>,1
57029a73,"22<*> <*>:<*>:<*> INFO Job: Task Id : attempt_<*>_<*>_r_<*>_<*>, Status : FAILED<*> <*> <*> at <*> at <*> at <*> at <*> at <*> at <*>    at <*>  <*>          at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> ... <*> more<*>",4
87254e64,[<*> <*>:<*>:<*>.<*>]File does not exist: hdfs:<*> File does not exist: hdfs:<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*>,1
f551cd29,22<*> <*>:<*>:<*> INFO Job: Job job_<*>_<*> failed with state FAILED due to: Task failed ,3
8e52464d,Job failed as tasks failed. failedMaps:<*> failedReduces:<*> killedMaps:<*> killedReduces: <*><*>,3
95db2126,"22<*> <*>:<*>:<*> INFO Job: Counters: <*><*> File System Counters<*> FILE: Number of bytes read<*><*><*> FILE: Number of bytes written<*><*><*> FILE: Number of read operations<*><*><*> FILE: Number of large read operations<*><*><*> FILE: Number of write operations<*><*><*> HDFS: Number of bytes read<*><*><*> HDFS: Number of bytes written<*><*><*> HDFS: Number of read operations<*><*><*> HDFS: Number of large read operations<*><*><*> HDFS: Number of write operations<*><*><*> Job Counters <*> Failed reduce tasks<*><*><*> Launched map tasks<*><*><*> Launched reduce tasks<*><*><*> Data<*>local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total time spent by all reduce tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total vcore<*>milliseconds taken by all reduce tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all reduce tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Map output bytes<*><*><*> Map output materialized bytes<*><*><*> Input split bytes<*><*><*> Combine input records<*><*><*> Spilled Records<*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*> File Input Format Counters <*> Bytes Read<*><*><*> in thread ""main"" <*> Job failed!<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*>",2
4c7f39dc,"22<*> <*>:<*>:<*> WARN MahoutDriver: No trainnb.props found on classpath, will use command<*>line arguments only<*>",2
4f24d7ac,"22<*> <*>:<*>:<*> INFO AbstractJob: Command line arguments: {<*>alphaI<*>[<*>.<*>], <*>endPhase<*>[<*>], <*>input<*>[hdfs:<*> <*>labelIndex<*>[hdfs:<*> <*>output<*>[hdfs:<*> <*>overwrite<*>null, <*>startPhase<*>[<*>], <*>tempDir<*>[hdfs:<*>",2
4410efb0,"22<*> <*>:<*>:<*> INFO JobSubmitter: Cleaning up the staging area <*> in thread ""main"" <*> Input path does not exist: hdfs:<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*>      at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*>                at <*> Method)<*> at <*> at <*> at <*> at <*> at <*>",2
0e0e4a22,finish HadoopBayes bench<*>,4
a5ee3eed,start HadoopPagerank bench<*>,3
375c63d2,rm: `hdfs:<*> No such file or directory<*>,1
fc158a0a,Submit MapReduce Job: <*> <*>config <*> jar <*> pegasus.PagerankNaive hdfs:<*> hdfs:<*> <*> <*> <*> nosym new<*>,3
146f8eb0,<*>[PEGASUS: A Peta<*>Scale Graph Mining System]<*><*>,3
2860b7fa,"[PEGASUS] Computing PageRank. Max iteration <*> <*>, threshold <*> <*>.<*>, cur_iteration<*><*><*>",3
2a4245ba,Creating initial pagerank <*>,3
dac20c7b,mysql> select * from mysql.general_log;<*> event_time <*> user_host <*> thread_id <*> server_id <*> command_type <*> argument <*><*> <*> <*>:<*>:<*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Query <*> select * from MyDB.test <*><*> <*> <*>:<*>:<*> <*> [root] @ localhost [] <*> <*> <*> <*> <*> Connect <*> root@localhost on <*><*> <*> <*>:<*>:<*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Query <*> KILL QUERY <*> <*><*> <*> <*>:<*>:<*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Quit <*> <*><*> <*> <*>:<*>:<*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Query <*> select * from mysql.general_log <*><*>,1
32c255cf,"22<*> <*>:<*>:<*> INFO Job: Task Id : attempt_<*>_<*>_r_<*>_<*>, Status : FAILED<*> <*> File does not exist: <*> (inode <*>) Holder DFSClient_attempt_<*>_<*>_r_<*>_<*>_<*>_<*> does not have any open files.<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> by: <*> File does not exist: <*> (inode <*>) Holder DFSClient_attempt_<*>_<*>_r_<*>_<*>_<*>_<*> does not have any open files.<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Source)<*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Source)<*> at <*> ... <*> more<*>",1
26f62209,"Traceback (most recent call last):<*> File ""<stdin>"", line <*>, in <module><*> list index out of range<*>",1
d1514903,22<*> <*>:<*>:<*> INFO mapreduce.Job: Counters: <*><*> File System Counters<*> FILE: Number of bytes read<*><*><*> FILE: Number of bytes written<*><*><*> FILE: Number of read operations<*><*><*> FILE: Number of large read operations<*><*><*> FILE: Number of write operations<*><*><*> HDFS: Number of bytes read<*><*><*> HDFS: Number of bytes written<*><*><*> HDFS: Number of read operations<*><*><*> HDFS: Number of large read operations<*><*><*> HDFS: Number of write operations<*><*><*> Job Counters <*> Launched map tasks<*><*><*> Launched reduce tasks<*><*><*> Data<*>local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total time spent by all reduce tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total vcore<*>milliseconds taken by all reduce tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all reduce tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Map output bytes<*><*><*> Map output materialized bytes<*><*><*> Input split bytes<*><*><*> Combine input records<*><*><*> Combine output records<*><*><*> Reduce input groups<*><*><*> Reduce shuffle bytes<*><*><*> Reduce input records<*><*><*> Reduce output records<*><*><*> Spilled Records<*><*><*> Shuffled Maps <*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*> Peak Reduce Physical memory (bytes)<*><*><*> Peak Reduce Virtual memory (bytes)<*><*><*> Shuffle Errors<*> BAD_ID<*><*><*> CONNECTION<*><*><*> IO_ERROR<*><*><*> WRONG_LENGTH<*><*><*> WRONG_MAP<*><*><*> WRONG_REDUCE<*><*><*> File Input Format Counters <*> Bytes Read<*><*><*> File Output Format Counters <*> Bytes Written<*><*><*> pegasus.PagerankNaive$PrCounters<*> CONVERGE_CHECK<*><*><*>,1
6557e869,"Iteration <*> <*>, changed reducer <*> <*><*>",1
d4479b12,[PEGASUS] PageRank computed.<*>,1
f88dcb1f,[PEGASUS] The final PageRanks are in the HDFS pr_vector.<*>,1
a44516e5,finish HadoopPagerank bench<*>,1
7a58a44f,22<*> <*>:<*>:<*> INFO HadoopUtil: Deleting hdfs:<*>,1
8a9aa103,22<*> <*>:<*>:<*> INFO SparseVectorsFromSequenceFiles: Calculating IDF<*>,1
7603e08f,"22<*> <*>:<*>:<*> INFO Job: Task Id : attempt_<*>_<*>_m_<*>_<*>, Status : FAILED<*> <*> File does not exist: hdfs:<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*>    at <*> at <*> at <*>",9
1b508de7,"22<*> <*>:<*>:<*> INFO Job: Counters: <*><*> Job Counters <*> Failed map tasks<*><*><*> Killed map tasks<*><*><*> Killed reduce tasks<*><*><*> Launched map tasks<*><*><*> Other local map tasks<*><*><*> Data<*>local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Map<*>Reduce Framework<*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> in thread ""main"" <*> Job failed!<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*>",1
3e311a29,"Traceback (most recent call last):<*> File ""<stdin>"", line <*>, in <module><*> No module named 'nonexistmodule' .<*>",1
5b856f9e,"Traceback (most recent call last):<*> File ""<stdin>"", line <*>, in <module><*> File ""<stdin>"", line <*>, in demo<*> File ""<stdin>"", line <*>, in demo<*> File ""<stdin>"", line <*>, in demo<*> [Previous line repeated <*> more times]<*> File ""<stdin>"", line <*>, in demo<*> maximum recursion depth exceeded in comparison<*>",1
0ff70e4e,Type Date Time Input_data_size Duration(s) Throughput(bytes<*> Throughput<*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*>,659
70db2767,Type Date Time Input_data_size Duration(s) Throughput(bytes<*> Throughput<*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*>,647
908131d3,Type Date Time Input_data_size Duration(s) Throughput(bytes<*> Throughput<*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*>,694
