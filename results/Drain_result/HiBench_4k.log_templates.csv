EventId,EventTemplate,Occurrences
0274f4f8,nohup: ignoring input<*>,3
64ca1ffb,<*> Start obtain and cache Kerberos ticket<*>granting ticket <*><*>,3
678301a5,Password for team7@BIGDATA: <*>,3
457d0ad2,<*> Start execute hibench test <*><*>,3
d223c8ec,mkdir: cannot create directory <*> File exists<*>,34
4f64db05,Start changing datasize ...<*>,3
370ddbe5,Creating <*> data ...<*>,5
34eba875,patching args<*><*>,32
018e820f,Parsing conf: <*>,128
ee3dfcce,probe sleep jar: <*>,32
acb15856,"ERROR, execute cmd: '( <*> node <*>list <*>> <*> <*> grep RUNNING )' timedout.<*> STDOU<*> STDER<*> Please check!<*>",1
10296da9,"Traceback (most recent call last):<*> File ""<*> line <*>, in <module><*> load_config(conf_root, workload_configFile, workload_folder, patching_config)<*> File ""<*> line <*>, in load_config<*> generate_optional_value()<*> File ""<*> line <*>, in generate_optional_value<*> probe_masters_slaves_hostnames()<*> File ""<*> line <*>, in probe_masters_slaves_hostnames<*> probe_masters_slaves_by_Yarn()<*> File ""<*> line <*>, in probe_masters_slaves_by_Yarn<*> assert <*>, ""Get workers from yarn<*>site.xml page failed, reason:%s\nplease set <*> and <*> manually"" % e<*> Get workers from yarn<*>site.xml page failed, reason:( <*> node <*>list <*>> <*> <*> grep RUNNING ) executed timedout for <*> seconds<*>",1
7740a8ad,please set <*> and <*> manually<*>,1
6ccf3d00,<*> line <*>: .: filename argument required<*>,1
5764a193,start <*> bench<*>,31
31eb8a06,hdfs rm <*>r: <*> <*>config <*> fs <*>rm <*>r <*>skipTrash hdfs:<*>,31
05d29f89,Deleted hdfs:<*>,30
dfa14b5d,Submit MapReduce Job: <*> <*>config <*> jar <*> teragen <*>D <*> <*>D <*> <*>D <*> <*> hdfs:<*>,2
897eb8fa,22<*> <*>:<*>:<*> INFO <*> Connecting to Application History server at <*>,51
ae73fdd8,"22<*> <*>:<*>:<*> INFO <*> Created token for team7: HDFS_DELEGATION_TOKEN owner<*>team7@BIGDATA, renewer<*>yarn, realUser<*>, issueDate<*><*>, maxDate<*><*>, sequenceNumber<*><*>, masterKeyId<*><*> on ha<*>hdfs:cuhkcluster<*>",44
90fb6e17,22<*> <*>:<*>:<*> INFO <*> Getting new token from http:<*> renewer:rm<*>,44
97ed0447,"22<*> <*>:<*>:<*> INFO <*> New token received: (Kind: kms<*>dt, Service: <*> Ident: (kms<*>dt owner<*>team7, renewer<*>yarn, realUser<*>, issueDate<*><*>, maxDate<*><*>, sequenceNumber<*><*>, masterKeyId<*><*>))<*>",44
6689224c,"22<*> <*>:<*>:<*> INFO <*> Got dt for hdfs:<*> Kind: HDFS_DELEGATION_TOKEN, Service: ha<*>hdfs:cuhkcluster, Ident: (token for team7: HDFS_DELEGATION_TOKEN owner<*>team7@BIGDATA, renewer<*>yarn, realUser<*>, issueDate<*><*>, maxDate<*><*>, sequenceNumber<*><*>, masterKeyId<*><*>)<*>",44
f3da8fe8,"22<*> <*>:<*>:<*> INFO <*> Got dt for hdfs:<*> Kind: kms<*>dt, Service: <*> Ident: (kms<*>dt owner<*>team7, renewer<*>yarn, realUser<*>, issueDate<*><*>, maxDate<*><*>, sequenceNumber<*><*>, masterKeyId<*><*>)<*>",44
29d1c5e0,22<*> <*>:<*>:<*> INFO <*> Disabling Erasure Coding for path: <*>,44
7d26730c,22<*> <*>:<*>:<*> INFO terasort.TeraGen: Generating <*> using <*><*>,2
20c96639,22<*> <*>:<*>:<*> INFO <*> number of splits:<*><*>,42
20413351,22<*> <*>:<*>:<*> INFO <*> Submitting tokens for job: job_<*>_<*><*>,42
7da86046,"22<*> <*>:<*>:<*> INFO <*> Executing with tokens: [Kind: HDFS_DELEGATION_TOKEN, Service: ha<*>hdfs:cuhkcluster, Ident: (token for team7: HDFS_DELEGATION_TOKEN owner<*>team7@BIGDATA, renewer<*>yarn, realUser<*>, issueDate<*><*>, maxDate<*><*>, sequenceNumber<*><*>, masterKeyId<*><*>), Kind: kms<*>dt, Service: <*> Ident: (kms<*>dt owner<*>team7, renewer<*>yarn, realUser<*>, issueDate<*><*>, maxDate<*><*>, sequenceNumber<*><*>, masterKeyId<*><*>)]<*>",42
6b2d7d58,22<*> <*>:<*>:<*> INFO <*> found resource resource<*>types.xml at file:<*>,32
88ee3db3,22<*> <*>:<*>:<*> INFO <*> Timeline service address: <*>,43
5687db9a,22<*> <*>:<*>:<*> INFO <*> Submitted application application_<*>_<*><*>,42
8b00557e,22<*> <*>:<*>:<*> INFO <*> The url to track the job: http:<*>,43
12416bdf,22<*> <*>:<*>:<*> INFO <*> Running job: job_<*>_<*><*>,43
cc229564,22<*> <*>:<*>:<*> INFO <*> Job job_<*>_<*> running in uber mode : <*><*>,43
e3e2bb39,22<*> <*>:<*>:<*> INFO <*> map <*>% reduce <*>%<*>,258
9096a907,22<*> <*>:<*>:<*> INFO <*> Job job_<*>_<*> completed successfully<*>,41
3f55ddf0,22<*> <*>:<*>:<*> INFO mapreduce.Job: Counters: <*><*> File System Counters<*> FILE: Number of bytes read<*><*><*> FILE: Number of bytes written<*><*><*> FILE: Number of read operations<*><*><*> FILE: Number of large read operations<*><*><*> FILE: Number of write operations<*><*><*> HDFS: Number of bytes read<*><*><*> HDFS: Number of bytes written<*><*><*> HDFS: Number of read operations<*><*><*> HDFS: Number of large read operations<*><*><*> HDFS: Number of write operations<*><*><*> Job Counters <*> Launched map tasks<*><*><*> Other local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Input split bytes<*><*><*> Spilled Records<*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*> <*> <*> File Input Format Counters <*> Bytes Read<*><*><*> File Output Format Counters <*> Bytes Written<*><*><*>,4
810a13a1,finish <*> bench<*>,32
47df6e7d,total used free shared buff<*> available<*> <*> <*> <*> <*> <*> <*><*> <*> <*> <*> <*> <*> <*> <*>,1
abdeec3d,Start changing mapper and reducer ...<*>,22
e8b0ae87,"<*>Start execute hibench datasize: <*> mapper: <*>, reducer: <*> <*><*>",22
a83566c9,Running <*> test ...<*>,27
6d8179c3,USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND<*> <*> <*>.<*> <*>.<*> <*> <*> tty1 Ss<*> <*>:<*> <*>:<*> <*> <*>noclear tty1 linux<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> sh run_terasort.sh<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> R <*>:<*> <*>:<*> ps au<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*>,12
e4428b5b,hdfs du <*>s: <*> <*>config <*> fs <*>du <*>s hdfs:<*>,27
f84e04d5,Submit MapReduce Job: <*> <*>config <*> jar <*> terasort <*>D <*> <*>D <*> hdfs:<*> hdfs:<*>,22
955f88c7,22<*> <*>:<*>:<*> INFO <*> <*>,46
68430c48,22<*> <*>:<*>:<*> INFO <*> Total input files to process : <*><*>,40
13ad3876,Spent <*> computing base<*>splits.<*>,22
ff975c27,Spent <*> computing TeraScheduler splits.<*>,22
dad1f533,Computing input splits took <*>,22
68e224eb,Sampling <*> splits of <*><*>,22
c47ca505,Making <*> from <*> sampled records<*>,22
9c3e190a,Computing parititions took <*>,22
592be439,Spent <*> computing partitions.<*>,22
8a2803a2,22<*> <*>:<*>:<*> INFO mapreduce.Job: Counters: <*><*> File System Counters<*> FILE: Number of bytes read<*><*><*> FILE: Number of bytes written<*><*><*> FILE: Number of read operations<*><*><*> FILE: Number of large read operations<*><*><*> FILE: Number of write operations<*><*><*> HDFS: Number of bytes read<*><*><*> HDFS: Number of bytes written<*><*><*> HDFS: Number of read operations<*><*><*> HDFS: Number of large read operations<*><*><*> HDFS: Number of write operations<*><*><*> Job Counters <*> Launched map tasks<*><*><*> Launched reduce tasks<*><*><*> Other local map tasks<*><*><*> Data<*>local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total time spent by all reduce tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total vcore<*>milliseconds taken by all reduce tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all reduce tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Map output bytes<*><*><*> Map output materialized bytes<*><*><*> Input split bytes<*><*><*> Combine input records<*><*><*> Combine output records<*><*><*> Reduce input groups<*><*><*> Reduce shuffle bytes<*><*><*> Reduce input records<*><*><*> Reduce output records<*><*><*> Spilled Records<*><*><*> Shuffled Maps <*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*> Peak Reduce Physical memory (bytes)<*><*><*> Peak Reduce Virtual memory (bytes)<*><*><*> Shuffle Errors<*> BAD_ID<*><*><*> CONNECTION<*><*><*> IO_ERROR<*><*><*> WRONG_LENGTH<*><*><*> WRONG_MAP<*><*><*> WRONG_REDUCE<*><*><*> File Input Format Counters <*> Bytes Read<*><*><*> File Output Format Counters <*> Bytes Written<*><*><*>,13
93b2c346,top: failed tty get<*>,16
fb3b04d3,Filesystem 1K<*>blocks Used Available Use% Mounted on<*> <*> <*> <*> <*>% <*> <*> <*> <*> <*>% <*> <*> <*> <*> <*>% <*> <*> <*> <*> <*>% <*> <*> <*> <*> <*>% <*> <*> <*> <*> <*>% <*> <*> <*> <*> <*>% <*>,16
37837b14,total used free shared buff<*> available<*> <*> <*> <*> <*> <*> <*><*> <*> <*> <*> <*> <*> <*><*>,15
90ddd592,"USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND<*> <*> <*>.<*> <*>.<*> <*> <*> tty1 Ss<*> <*>:<*> <*>:<*> <*> <*>noclear tty1 linux<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> sh run_terasort.sh<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Sl <*>:<*> <*>:<*> python2 <*> HadoopTerasort <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> ssh <*> python <*>u <*>c ""exec('\nimport time, os, sys, socket, <*> <*> <*> func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines() if x.startswith(\""cpu\"")]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines()]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n blocks <*> os.listdir(\""<*> <*> for x in f.readlines() if x.split()[<*>] in blocks and not <*> and x.split()[<*>]!<*>\""<*>\""]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n mem <*> dict([(a, <*> for a, b in [x.split(\"":\"") for x in f.readlines()]])\n <*> for field in [\""MemTotal\"", \""Buffers\"", \""Cached\"", \""MemFree\"", \""Mapped\""]])<*>chr(<*>))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\nwhile <*>:\n s2.send((\""<*>SEP<*>%s\"" % time.time())<*>chr(<*>))\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n s2.send(\""<*>SEP<*>#end\""<*>chr(<*>))\n <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> ssh <*> python <*>u <*>c ""exec('\nimport time, os, sys, socket, <*> <*> <*> func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines() if x.startswith(\""cpu\"")]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines()]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n blocks <*> os.listdir(\""<*> <*> for x in f.readlines() if x.split()[<*>] in blocks and not <*> and x.split()[<*>]!<*>\""<*>\""]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n mem <*> dict([(a, <*> for a, b in [x.split(\"":\"") for x in f.readlines()]])\n <*> for field in [\""MemTotal\"", \""Buffers\"", \""Cached\"", \""MemFree\"", \""Mapped\""]])<*>chr(<*>))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\nwhile <*>:\n s2.send((\""<*>SEP<*>%s\"" % time.time())<*>chr(<*>))\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n s2.send(\""<*>SEP<*>#end\""<*>chr(<*>))\n <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> ssh <*> python <*>u <*>c ""exec('\nimport time, os, sys, socket, <*> <*> <*> func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines() if x.startswith(\""cpu\"")]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines()]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n blocks <*> os.listdir(\""<*> <*> for x in f.readlines() if x.split()[<*>] in blocks and not <*> and x.split()[<*>]!<*>\""<*>\""]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n mem <*> dict([(a, <*> for a, b in [x.split(\"":\"") for x in f.readlines()]])\n <*> for field in [\""MemTotal\"", \""Buffers\"", \""Cached\"", \""MemFree\"", \""Mapped\""]])<*>chr(<*>))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\nwhile <*>:\n s2.send((\""<*>SEP<*>%s\"" % time.time())<*>chr(<*>))\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n s2.send(\""<*>SEP<*>#end\""<*>chr(<*>))\n <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> ssh <*> python <*>u <*>c ""exec('\nimport time, os, sys, socket, <*> <*> <*> func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines() if x.startswith(\""cpu\"")]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines()]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n blocks <*> os.listdir(\""<*> <*> for x in f.readlines() if x.split()[<*>] in blocks and not <*> and x.split()[<*>]!<*>\""<*>\""]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n mem <*> dict([(a, <*> for a, b in [x.split(\"":\"") for x in f.readlines()]])\n <*> for field in [\""MemTotal\"", \""Buffers\"", \""Cached\"", \""MemFree\"", \""Mapped\""]])<*>chr(<*>))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\nwhile <*>:\n s2.send((\""<*>SEP<*>%s\"" % time.time())<*>chr(<*>))\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n s2.send(\""<*>SEP<*>#end\""<*>chr(<*>))\n <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> R <*>:<*> <*>:<*> ps au<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*>",2
3ed32001,22<*> <*>:<*>:<*> INFO mapreduce.Job: Counters: <*><*> File System Counters<*> FILE: Number of bytes read<*><*><*> FILE: Number of bytes written<*><*><*> FILE: Number of read operations<*><*><*> FILE: Number of large read operations<*><*><*> FILE: Number of write operations<*><*><*> HDFS: Number of bytes read<*><*><*> HDFS: Number of bytes written<*><*><*> HDFS: Number of read operations<*><*><*> HDFS: Number of large read operations<*><*><*> HDFS: Number of write operations<*><*><*> Job Counters <*> Launched map tasks<*><*><*> Launched reduce tasks<*><*><*> Data<*>local map tasks<*><*><*> Rack<*>local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total time spent by all reduce tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total vcore<*>milliseconds taken by all reduce tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all reduce tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Map output bytes<*><*><*> Map output materialized bytes<*><*><*> Input split bytes<*><*><*> Combine input records<*><*><*> Combine output records<*><*><*> Reduce input groups<*><*><*> Reduce shuffle bytes<*><*><*> Reduce input records<*><*><*> Reduce output records<*><*><*> Spilled Records<*><*><*> Shuffled Maps <*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*> Peak Reduce Physical memory (bytes)<*><*><*> Peak Reduce Virtual memory (bytes)<*><*><*> Shuffle Errors<*> BAD_ID<*><*><*> CONNECTION<*><*><*> IO_ERROR<*><*><*> WRONG_LENGTH<*><*><*> WRONG_MAP<*><*><*> WRONG_REDUCE<*><*><*> File Input Format Counters <*> Bytes Read<*><*><*> File Output Format Counters <*> Bytes Written<*><*><*>,4
01b4b276,USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND<*> <*> <*>.<*> <*>.<*> <*> <*> tty1 Ss<*> <*>:<*> <*>:<*> <*> <*>noclear tty1 linux<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> sh run_terasort.sh<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> R <*>:<*> <*>:<*> python2 <*> HadoopTerasort <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> R <*>:<*> <*>:<*> ps au<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*>,1
254e71b8,PID USER PR NI VIRT RES SHR ST %CPU %MEM TIME<*> COMMAND <*> mqy <*> <*> <*>.3g <*> <*> R <*>.<*> <*>.<*> <*>:<*> node <*> wwmm <*> <*> <*>.1g <*>.6g <*>.2g R <*>.<*> <*>.<*> <*>:<*>.<*> python <*> wwmm <*> <*> <*>.5g <*>.4g <*>.2g S <*>.<*> <*>.<*> <*>:<*>.<*> python <*> root <*> <*> <*> <*> <*> R <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> yyl <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> nvidia<*>smi <*> xjjl <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> xjjl <*> <*> <*> <*> <*> R <*>.<*> <*>.<*> <*>:<*>.<*> top <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> containerd <*> hyy <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> mysql <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> mysqld <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> rcu_sched <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> irq<*> <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> <*> hyy <*> <*> <*>.0g <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> java <*>,1
001722e4,"USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND<*> <*> <*>.<*> <*>.<*> <*> <*> tty1 Ss<*> <*>:<*> <*>:<*> <*> <*>noclear tty1 linux<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> sh run_terasort.sh<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Sl <*>:<*> <*>:<*> python2 <*> HadoopTerasort <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> ssh <*> python <*>u <*>c ""exec('\nimport time, os, sys, socket, <*> <*> <*> func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines() if x.startswith(\""cpu\"")]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines()]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n blocks <*> os.listdir(\""<*> <*> for x in f.readlines() if x.split()[<*>] in blocks and not <*> and x.split()[<*>]!<*>\""<*>\""]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n mem <*> dict([(a, <*> for a, b in [x.split(\"":\"") for x in f.readlines()]])\n <*> for field in [\""MemTotal\"", \""Buffers\"", \""Cached\"", \""MemFree\"", \""Mapped\""]])<*>chr(<*>))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\nwhile <*>:\n s2.send((\""<*>SEP<*>%s\"" % time.time())<*>chr(<*>))\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n s2.send(\""<*>SEP<*>#end\""<*>chr(<*>))\n <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> ssh <*> python <*>u <*>c ""exec('\nimport time, os, sys, socket, <*> <*> <*> func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines() if x.startswith(\""cpu\"")]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines()]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n blocks <*> os.listdir(\""<*> <*> for x in f.readlines() if x.split()[<*>] in blocks and not <*> and x.split()[<*>]!<*>\""<*>\""]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n mem <*> dict([(a, <*> for a, b in [x.split(\"":\"") for x in f.readlines()]])\n <*> for field in [\""MemTotal\"", \""Buffers\"", \""Cached\"", \""MemFree\"", \""Mapped\""]])<*>chr(<*>))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\nwhile <*>:\n s2.send((\""<*>SEP<*>%s\"" % time.time())<*>chr(<*>))\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n s2.send(\""<*>SEP<*>#end\""<*>chr(<*>))\n <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> ssh <*> python <*>u <*>c ""exec('\nimport time, os, sys, socket, <*> <*> <*> func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines() if x.startswith(\""cpu\"")]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines()]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n blocks <*> os.listdir(\""<*> <*> for x in f.readlines() if x.split()[<*>] in blocks and not <*> and x.split()[<*>]!<*>\""<*>\""]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n mem <*> dict([(a, <*> for a, b in [x.split(\"":\"") for x in f.readlines()]])\n <*> for field in [\""MemTotal\"", \""Buffers\"", \""Cached\"", \""MemFree\"", \""Mapped\""]])<*>chr(<*>))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\nwhile <*>:\n s2.send((\""<*>SEP<*>%s\"" % time.time())<*>chr(<*>))\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n s2.send(\""<*>SEP<*>#end\""<*>chr(<*>))\n <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> S <*>:<*> <*>:<*> ssh <*> python <*>u <*>c ""exec('\nimport time, os, sys, socket, <*> <*> <*> func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines() if x.startswith(\""cpu\"")]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> for x in f.readlines()]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n blocks <*> os.listdir(\""<*> <*> for x in f.readlines() if x.split()[<*>] in blocks and not <*> and x.split()[<*>]!<*>\""<*>\""]))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n mem <*> dict([(a, <*> for a, b in [x.split(\"":\"") for x in f.readlines()]])\n <*> for field in [\""MemTotal\"", \""Buffers\"", \""Cached\"", \""MemFree\"", \""Mapped\""]])<*>chr(<*>))\n \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\ndef func_<*>():\n with open(\""<*> as f:\n <*> \n s2.send(\""<*>SEP<*><*>\""<*>chr(<*>))\nwhile <*>:\n s2.send((\""<*>SEP<*>%s\"" % time.time())<*>chr(<*>))\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n func_<*>()\n s2.send(\""<*>SEP<*>#end\""<*>chr(<*>))\n <*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> R <*>:<*> <*>:<*> ps au<*> <*> <*>.<*> <*>.<*> <*> <*> pts<*> Ss<*> <*>:<*> <*>:<*> <*> <*>init<*>file <*>",1
7d19ee4f,PID USER PR NI VIRT RES SHR ST %CPU %MEM TIME<*> COMMAND <*> mqy <*> <*> <*>.3g <*> <*> R <*>.<*> <*>.<*> <*>:<*> node <*> wwmm <*> <*> <*>.1g <*>.6g <*>.2g R <*>.<*> <*>.<*> <*>:<*>.<*> python <*> wwmm <*> <*> <*>.5g <*>.4g <*>.2g S <*>.<*> <*>.<*> <*>:<*>.<*> python <*> root <*> <*> <*> <*> <*> R <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> yyl <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> nvidia<*>smi <*> xjjl <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> xjjl <*> <*> <*> <*> <*> R <*>.<*> <*>.<*> <*>:<*>.<*> top <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> containerd <*> hyy <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> mysql <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> mysqld <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> rcu_sched <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> irq<*> <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> <*> hyy <*> <*> <*>.0g <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> java <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> hyy <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> sshd <*> jiaqi <*> <*> <*>.3g <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*>,1
ff8baae8,22<*> <*>:<*>:<*> INFO mapreduce.Job: Counters: <*><*> File System Counters<*> FILE: Number of bytes read<*><*><*> FILE: Number of bytes written<*><*><*> FILE: Number of read operations<*><*><*> FILE: Number of large read operations<*><*><*> FILE: Number of write operations<*><*><*> HDFS: Number of bytes read<*><*><*> HDFS: Number of bytes written<*><*><*> HDFS: Number of read operations<*><*><*> HDFS: Number of large read operations<*><*><*> HDFS: Number of write operations<*><*><*> Job Counters <*> Launched map tasks<*><*><*> Launched reduce tasks<*><*><*> Other local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total time spent by all reduce tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total vcore<*>milliseconds taken by all reduce tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all reduce tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Map output bytes<*><*><*> Map output materialized bytes<*><*><*> Input split bytes<*><*><*> Combine input records<*><*><*> Combine output records<*><*><*> Reduce input groups<*><*><*> Reduce shuffle bytes<*><*><*> Reduce input records<*><*><*> Reduce output records<*><*><*> Spilled Records<*><*><*> Shuffled Maps <*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*> Peak Reduce Physical memory (bytes)<*><*><*> Peak Reduce Virtual memory (bytes)<*><*><*> Shuffle Errors<*> BAD_ID<*><*><*> CONNECTION<*><*><*> IO_ERROR<*><*><*> WRONG_LENGTH<*><*><*> WRONG_MAP<*><*><*> WRONG_REDUCE<*><*><*> File Input Format Counters <*> Bytes Read<*><*><*> File Output Format Counters <*> Bytes Written<*><*><*>,3
9619e968,PID USER PR NI VIRT RES SHR ST %CPU %MEM TIME<*> COMMAND <*> mqy <*> <*> <*>.3g <*> <*> R <*>.<*> <*>.<*> <*>:<*> node <*> wwmm <*> <*> <*>.1g <*>.6g <*>.2g R <*>.<*> <*>.<*> <*>:<*>.<*> python <*> wwmm <*> <*> <*>.5g <*>.4g <*>.2g S <*>.<*> <*>.<*> <*>:<*>.<*> python <*> yyl <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> nvidia<*>smi <*> xjjl <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> xjjl <*> <*> <*> <*> <*> R <*>.<*> <*>.<*> <*>:<*>.<*> top <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> containerd <*> hyy <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> mysql <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> mysqld <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> rcu_sched <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> irq<*> <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> <*> hyy <*> <*> <*>.0g <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> java <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> hyy <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> sshd <*> jiaqi <*> <*> <*>.3g <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*>,1
11cf4376,22<*> <*>:<*>:<*> INFO <*> Counters: <*><*> File System Counters<*> FILE: Number of bytes read<*><*><*> FILE: Number of bytes written<*><*><*> FILE: Number of read operations<*><*><*> FILE: Number of large read operations<*><*><*> FILE: Number of write operations<*><*><*> HDFS: Number of bytes read<*><*><*> HDFS: Number of bytes written<*><*><*> HDFS: Number of read operations<*><*><*> HDFS: Number of large read operations<*><*><*> HDFS: Number of write operations<*><*><*> Job Counters <*> Launched map tasks<*><*><*> Launched reduce tasks<*><*><*> Data<*>local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total time spent by all reduce tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total vcore<*>milliseconds taken by all reduce tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all reduce tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Map output bytes<*><*><*> Map output materialized bytes<*><*><*> Input split bytes<*><*><*> Combine input records<*><*><*> Combine output records<*><*><*> Reduce input groups<*><*><*> Reduce shuffle bytes<*><*><*> Reduce input records<*><*><*> Reduce output records<*><*><*> Spilled Records<*><*><*> Shuffled Maps <*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*> Peak Reduce Physical memory (bytes)<*><*><*> Peak Reduce Virtual memory (bytes)<*><*><*> Shuffle Errors<*> BAD_ID<*><*><*> CONNECTION<*><*><*> IO_ERROR<*><*><*> WRONG_LENGTH<*><*><*> WRONG_MAP<*><*><*> WRONG_REDUCE<*><*><*> File Input Format Counters <*> Bytes Read<*><*><*> File Output Format Counters <*> Bytes Written<*><*><*>,11
c993e55c,PID USER PR NI VIRT RES SHR ST %CPU %MEM TIME<*> COMMAND <*> mqy <*> <*> <*>.3g <*> <*> R <*>.<*> <*>.<*> <*>:<*> node <*> wwmm <*> <*> <*>.1g <*>.6g <*>.2g R <*>.<*> <*>.<*> <*>:<*>.<*> python <*> wwmm <*> <*> <*>.5g <*>.4g <*>.2g S <*>.<*> <*>.<*> <*>:<*>.<*> python <*> root <*> <*> <*> <*> <*> R <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> yyl <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> nvidia<*>smi <*> xjjl <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> xjjl <*> <*> <*> <*> <*> R <*>.<*> <*>.<*> <*>:<*>.<*> top <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> containerd <*> hyy <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> mysql <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> mysqld <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> rcu_sched <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> irq<*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> hyy <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> sshd <*> jiaqi <*> <*> <*>.3g <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*>,1
94eeb5eb,PID USER PR NI VIRT RES SHR ST %CPU %MEM TIME<*> COMMAND <*> mqy <*> <*> <*>.3g <*> <*> R <*>.<*> <*>.<*> <*>:<*> node <*> wwmm <*> <*> <*>.1g <*>.6g <*>.2g R <*>.<*> <*>.<*> <*>:<*>.<*> python <*> wwmm <*> <*> <*>.5g <*>.4g <*>.2g S <*>.<*> <*>.<*> <*>:<*>.<*> python <*> root <*> <*> <*> <*> <*> R <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> xjjl <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> xjjl <*> <*> <*> <*> <*> R <*>.<*> <*>.<*> <*>:<*>.<*> top <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> containerd <*> hyy <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> skyfall <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*> mysql <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> mysqld <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> rcu_sched <*> <*> root <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> irq<*> <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> <*> hyy <*> <*> <*>.0g <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> java <*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> root <*> <*> <*> <*> <*> I <*>.<*> <*>.<*> <*>:<*>.<*> kworker<*> <*> hyy <*> <*> <*> <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> sshd <*> jiaqi <*> <*> <*>.3g <*> <*> S <*>.<*> <*>.<*> <*>:<*>.<*> node <*>,1
48b5a586,Type Date Time Input_data_size Duration(s) Throughput(bytes<*> Throughput<*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*>,2
fc9e7958,Submit MapReduce Job: <*> <*>config <*> jar <*> HiBench.DataGen <*>t <*> <*>b hdfs:<*> <*>n Input <*>m <*> <*>r <*> <*>p <*> <*> <*> <*>o <*>,2
3c9a4ee2,22<*> <*>:<*>:<*> INFO HiBench.Dummy: Creating dummy file hdfs:<*> with <*> <*>,2
6ecfe59d,"curIndex: <*>, total: <*><*>",2
d6b2c743,22<*> <*>:<*>:<*> INFO HiBench.BayesData: creating bayes text data ... <*>,1
6a749441,22<*> <*>:<*>:<*> INFO <*> Running Job: Create <*> <*>,3
e39706fd,22<*> <*>:<*>:<*> INFO <*> <*> file hdfs:<*> as <*>,6
af31d8f1,22<*> <*>:<*>:<*> WARN mapreduce.JobResourceUploader: Hadoop command<*>line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.<*>,3
898aca3b,22<*> <*>:<*>:<*> INFO <*> Finished Running Job: Create <*> <*>,3
ade4b1ee,22<*> <*>:<*>:<*> INFO HiBench.BayesData: Closing bayes data <*>,1
6e8912d4,22<*> <*>:<*>:<*> INFO HiBench.PagerankData: <*> <*> data <*>,3
6b26e119,22<*> <*>:<*>:<*> INFO HiBench.PagerankData: Creating PageRank <*>,2
bacecebe,Export env: <*>,6
d2dd9328,MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.<*>,5
13003f12,"Running on hadoop, using <*> and HADOOP_CONF_DIR<*><*>",5
57056e9b,MAHOUT<*>JOB: <*>,5
09a00e08,22<*> <*>:<*>:<*> INFO SparseVectorsFromSequenceFiles: Maximum n<*>gram size is: <*><*>,3
7c2d5cd9,22<*> <*>:<*>:<*> INFO SparseVectorsFromSequenceFiles: Minimum LLR value: <*>.<*><*>,3
aa5e90e1,22<*> <*>:<*>:<*> INFO SparseVectorsFromSequenceFiles: Number of reduce tasks: <*><*>,3
b4d65167,22<*> <*>:<*>:<*> INFO SparseVectorsFromSequenceFiles: Tokenizing documents in hdfs:<*>,3
e091720a,"22<*> <*>:<*>:<*> INFO deprecation: <*> is deprecated. Instead, use <*>",44
60dc777c,22<*> <*>:<*>:<*> INFO Job: Counters: <*><*> File System Counters<*> FILE: Number of bytes read<*><*><*> FILE: Number of bytes written<*><*><*> FILE: Number of read operations<*><*><*> FILE: Number of large read operations<*><*><*> FILE: Number of write operations<*><*><*> HDFS: Number of bytes read<*><*><*> HDFS: Number of bytes written<*><*><*> HDFS: Number of read operations<*><*><*> HDFS: Number of large read operations<*><*><*> HDFS: Number of write operations<*><*><*> Job Counters <*> Launched map tasks<*><*><*> Data<*>local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Input split bytes<*><*><*> Spilled Records<*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*> File Input Format Counters <*> Bytes Read<*><*><*> File Output Format Counters <*> Bytes Written<*><*><*>,3
df022ef7,22<*> <*>:<*>:<*> INFO SparseVectorsFromSequenceFiles: Creating Term Frequency Vectors<*>,3
160199de,22<*> <*>:<*>:<*> INFO DictionaryVectorizer: Creating dictionary from hdfs:<*> and saving at hdfs:<*>,3
6064fb72,mysql> select * from mysql.general_log;<*> event_time <*> user_host <*> thread_id <*> server_id <*> command_type <*> argument <*><*> <*> <*>:<*>:<*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Query <*> show variables like 'general%' <*><*> <*> <*>:<*>:<*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Query <*> show variables like 'log_output' <*><*> <*> <*>:<*>:<*> <*> [root] @ localhost [] <*> <*> <*> <*> <*> Connect <*> root@localhost on <*><*> <*> <*>:<*>:<*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Query <*> KILL QUERY <*> <*><*> <*> <*>:<*>:<*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Quit <*> <*><*> <*> <*>:<*>:<*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Query <*> select * from mysql.general_log <*><*>,1
7b68b826,"22<*> <*>:<*>:<*> INFO Job: Task Id : attempt_<*>_<*>_r_<*>_<*>, Status : FAILED<*> <*> <*> at <*> at <*> at <*> at <*> at <*> at <*> <*> <*> <*> at <*> at <*> at <*> by: <*> File does not exist: hdfs:<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> ... <*> more<*>",3
8a02be66,"22<*> <*>:<*>:<*> INFO Job: Task Id : attempt_<*>_<*>_r_<*>_<*>, Status : FAILED<*> <*> <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> by: <*> File does not exist: hdfs:<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> ... <*> more<*>",1
91139137,[<*> <*>:<*>:<*>.<*>]File does not exist: hdfs:<*> File does not exist: hdfs:<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*>,1
da89e3da,22<*> <*>:<*>:<*> INFO Job: Job job_<*>_<*> failed with state FAILED due to: Task failed <*>,3
461f6d27,Job failed as tasks failed. failedMaps:<*> failedReduces:<*> killedMaps:<*> killedReduces: <*><*>,3
404d2e3d,"22<*> <*>:<*>:<*> INFO Job: Counters: <*><*> File System Counters<*> FILE: Number of bytes read<*><*><*> FILE: Number of bytes written<*><*><*> FILE: Number of read operations<*><*><*> FILE: Number of large read operations<*><*><*> FILE: Number of write operations<*><*><*> HDFS: Number of bytes read<*><*><*> HDFS: Number of bytes written<*><*><*> HDFS: Number of read operations<*><*><*> HDFS: Number of large read operations<*><*><*> HDFS: Number of write operations<*><*><*> Job Counters <*> Failed reduce tasks<*><*><*> Launched map tasks<*><*><*> Launched reduce tasks<*><*><*> Data<*>local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total time spent by all reduce tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total vcore<*>milliseconds taken by all reduce tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all reduce tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Map output bytes<*><*><*> Map output materialized bytes<*><*><*> Input split bytes<*><*><*> Combine input records<*><*><*> Spilled Records<*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*> File Input Format Counters <*> Bytes Read<*><*><*> in thread ""main"" <*> Job failed!<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*>",2
df859da8,"22<*> <*>:<*>:<*> WARN MahoutDriver: No trainnb.props found on classpath, will use command<*>line arguments only<*>",2
80fa6d57,"22<*> <*>:<*>:<*> INFO AbstractJob: Command line arguments: {<*>alphaI<*>[<*>.<*>], <*>endPhase<*>[<*>], <*>input<*>[hdfs:<*> <*>labelIndex<*>[hdfs:<*> <*>output<*>[hdfs:<*> <*>overwrite<*>null, <*>startPhase<*>[<*>], <*>tempDir<*>[hdfs:<*>",2
2ed99340,"22<*> <*>:<*>:<*> INFO JobSubmitter: Cleaning up the staging area <*> in thread ""main"" <*> Input path does not exist: hdfs:<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*>",1
b8f15c67,rm: `hdfs:<*> No such file or directory<*>,1
797ca4f2,Submit MapReduce Job: <*> <*>config <*> jar <*> pegasus.PagerankNaive hdfs:<*> hdfs:<*> <*> <*> <*> nosym new<*>,3
8b4919ae,<*>[PEGASUS: A Peta<*>Scale Graph Mining System]<*><*>,3
b177091c,"[PEGASUS] Computing PageRank. Max iteration <*> <*>, threshold <*> <*>.<*>, cur_iteration<*><*><*>",3
2f474ae5,Creating initial pagerank <*>,3
3843551f,mysql> select * from mysql.general_log;<*> event_time <*> user_host <*> thread_id <*> server_id <*> command_type <*> argument <*><*> <*> <*>:<*>:<*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Query <*> select * from MyDB.test <*><*> <*> <*>:<*>:<*> <*> [root] @ localhost [] <*> <*> <*> <*> <*> Connect <*> root@localhost on <*><*> <*> <*>:<*>:<*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Query <*> KILL QUERY <*> <*><*> <*> <*>:<*>:<*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Quit <*> <*><*> <*> <*>:<*>:<*> <*> root[root] @ localhost [] <*> <*> <*> <*> <*> Query <*> select * from mysql.general_log <*><*>,1
8427858a,22<*> <*>:<*>:<*> INFO MahoutDriver: Program took <*> ms (Minutes: <*>.<*>)<*>,2
c8bbee0d,"22<*> <*>:<*>:<*> INFO Job: Task Id : attempt_<*>_<*>_r_<*>_<*>, Status : FAILED<*> <*> File does not exist: <*> (inode <*>) Holder DFSClient_attempt_<*>_<*>_r_<*>_<*>_<*>_<*> does not have any open files.<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> by: <*> File does not exist: <*> (inode <*>) Holder DFSClient_attempt_<*>_<*>_r_<*>_<*>_<*>_<*> does not have any open files.<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Source)<*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Source)<*> at <*> ... <*> more<*>",1
763febe4,"Traceback (most recent call last):<*> File ""<stdin>"", line <*>, in <module><*> <*> <*> <*> <*> <*>",2
6769df44,22<*> <*>:<*>:<*> INFO mapreduce.Job: Counters: <*><*> File System Counters<*> FILE: Number of bytes read<*><*><*> FILE: Number of bytes written<*><*><*> FILE: Number of read operations<*><*><*> FILE: Number of large read operations<*><*><*> FILE: Number of write operations<*><*><*> HDFS: Number of bytes read<*><*><*> HDFS: Number of bytes written<*><*><*> HDFS: Number of read operations<*><*><*> HDFS: Number of large read operations<*><*><*> HDFS: Number of write operations<*><*><*> Job Counters <*> Launched map tasks<*><*><*> Launched reduce tasks<*><*><*> Data<*>local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total time spent by all reduce tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total vcore<*>milliseconds taken by all reduce tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all reduce tasks<*><*><*> Map<*>Reduce Framework<*> Map input records<*><*><*> Map output records<*><*><*> Map output bytes<*><*><*> Map output materialized bytes<*><*><*> Input split bytes<*><*><*> Combine input records<*><*><*> Combine output records<*><*><*> Reduce input groups<*><*><*> Reduce shuffle bytes<*><*><*> Reduce input records<*><*><*> Reduce output records<*><*><*> Spilled Records<*><*><*> Shuffled Maps <*><*><*> Failed Shuffles<*><*><*> Merged Map outputs<*><*><*> GC time elapsed (ms)<*><*><*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> Total committed heap usage (bytes)<*><*><*> Peak Map Physical memory (bytes)<*><*><*> Peak Map Virtual memory (bytes)<*><*><*> Peak Reduce Physical memory (bytes)<*><*><*> Peak Reduce Virtual memory (bytes)<*><*><*> Shuffle Errors<*> BAD_ID<*><*><*> CONNECTION<*><*><*> IO_ERROR<*><*><*> WRONG_LENGTH<*><*><*> WRONG_MAP<*><*><*> WRONG_REDUCE<*><*><*> File Input Format Counters <*> Bytes Read<*><*><*> File Output Format Counters <*> Bytes Written<*><*><*> pegasus.PagerankNaive$PrCounters<*> CONVERGE_CHECK<*><*><*>,1
b3f8d70a,"Iteration <*> <*>, changed reducer <*> <*><*>",1
3fae6721,[PEGASUS] PageRank computed.<*>,1
b6b6e164,[PEGASUS] The final PageRanks are in the HDFS pr_vector.<*>,1
50439efe,22<*> <*>:<*>:<*> INFO HadoopUtil: Deleting hdfs:<*>,1
8388f681,22<*> <*>:<*>:<*> INFO SparseVectorsFromSequenceFiles: Calculating IDF<*>,1
b088a3a1,"22<*> <*>:<*>:<*> INFO Job: Task Id : attempt_<*>_<*>_m_<*>_<*>, Status : FAILED<*> <*> File does not exist: hdfs:<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> <*> <*> <*> at <*> at <*> at <*>",9
9e8325e2,"22<*> <*>:<*>:<*> INFO Job: Counters: <*><*> Job Counters <*> Failed map tasks<*><*><*> Killed map tasks<*><*><*> Killed reduce tasks<*><*><*> Launched map tasks<*><*><*> Other local map tasks<*><*><*> Data<*>local map tasks<*><*><*> Total time spent by all maps in occupied slots (ms)<*><*><*> Total time spent by all reduces in occupied slots (ms)<*><*><*> Total time spent by all map tasks (ms)<*><*><*> Total vcore<*>milliseconds taken by all map tasks<*><*><*> Total megabyte<*>milliseconds taken by all map tasks<*><*><*> Map<*>Reduce Framework<*> CPU time spent (ms)<*><*><*> Physical memory (bytes) snapshot<*><*><*> Virtual memory (bytes) snapshot<*><*><*> in thread ""main"" <*> Job failed!<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*>",1
a2be53dc,"22<*> <*>:<*>:<*> INFO JobSubmitter: Cleaning up the staging area <*> in thread ""main"" <*> Input path does not exist: hdfs:<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> at <*> Method)<*> at <*> at <*> at <*> at <*> at <*>",1
d9ee4182,"Traceback (most recent call last):<*> File ""<stdin>"", line <*>, in <module><*> File ""<stdin>"", line <*>, in demo<*> File ""<stdin>"", line <*>, in demo<*> File ""<stdin>"", line <*>, in demo<*> [Previous line repeated <*> more times]<*> File ""<stdin>"", line <*>, in demo<*> maximum recursion depth exceeded in comparison<*>",1
552509e6,Type Date Time Input_data_size Duration(s) Throughput(bytes<*> Throughput<*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*>,659
98854228,Type Date Time Input_data_size Duration(s) Throughput(bytes<*> Throughput<*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*>,647
e213a881,Type Date Time Input_data_size Duration(s) Throughput(bytes<*> Throughput<*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*> <*> <*>:<*>:<*> <*> <*>.<*> <*> <*> <*>,694
