symbol: m5
template: INFO mapreduce.Job: Task Id : attempt_<*it>_<*it>_m_<*it>_<*it>, Status : FAILED
tid: 8cbf4685
174,attempt_1667571544727_0239_m_000000_0,,
||Error: org.apache.hadoop.hdfs.protocol.DSQuotaExceededException: The DiskSpace quota of /dataspace/team7 is exceeded: quota = 10737418240 B = 10 GB but diskspace consumed = 11049082274 B = 10.29 GB
|>	at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyStoragespaceQuota(DirectoryWithQuotaFeature.java:195)
|>	at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:222)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:1147)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:979)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:938)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addBlock(FSDirWriteFileOp.java:504)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.saveAllocatedBlock(FSDirWriteFileOp.java:771)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.storeAllocatedBlock(FSDirWriteFileOp.java:259)
|>	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2714)
|>	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
|>	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
|>	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
|>	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
|>	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
|>	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
|>	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
||	at java.security.AccessController.doPrivileged(Native Method)
|>	at javax.security.auth.Subject.doAs(Subject.java:422)
|>	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
|>	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
||	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
|>	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
|>	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
|>	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
|>	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
|>	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
|>	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
|>	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
|>	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
|>	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
||Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.DSQuotaExceededException): The DiskSpace quota of /dataspace/team7 is exceeded: quota = 10737418240 B = 10 GB but diskspace consumed = 11049082274 B = 10.29 GB
|>	at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyStoragespaceQuota(DirectoryWithQuotaFeature.java:195)
|>	at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:222)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:1147)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:979)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:938)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addBlock(FSDirWriteFileOp.java:504)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.saveAllocatedBlock(FSDirWriteFileOp.java:771)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.storeAllocatedBlock(FSDirWriteFileOp.java:259)
|>	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2714)
|>	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
|>	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
|>	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
|>	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
|>	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
|>	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
|>	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
||	at java.security.AccessController.doPrivileged(Native Method)
|>	at javax.security.auth.Subject.doAs(Subject.java:422)
|>	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
|>	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
|>	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
|>	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
|>	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
|>	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
|>	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
||	at com.sun.proxy.$Proxy13.addBlock(Unknown Source)
||	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:510)
||	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
|>	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
|>	at java.lang.reflect.Method.invoke(Method.java:498)
|>	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
|>	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
|>	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
|>	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
|>	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
||	at com.sun.proxy.$Proxy14.addBlock(Unknown Source)
||	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1078)
||	... 3 more
248,attempt_1667571544727_0239_m_000000_1,,
||Error: org.apache.hadoop.hdfs.protocol.DSQuotaExceededException: The DiskSpace quota of /dataspace/team7 is exceeded: quota = 10737418240 B = 10 GB but diskspace consumed = 11049082274 B = 10.29 GB
|>	at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyStoragespaceQuota(DirectoryWithQuotaFeature.java:195)
|>	at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:222)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:1147)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:979)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:938)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addBlock(FSDirWriteFileOp.java:504)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.saveAllocatedBlock(FSDirWriteFileOp.java:771)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.storeAllocatedBlock(FSDirWriteFileOp.java:259)
|>	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2714)
|>	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
|>	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
|>	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
|>	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
|>	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
|>	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
|>	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
||	at java.security.AccessController.doPrivileged(Native Method)
|>	at javax.security.auth.Subject.doAs(Subject.java:422)
|>	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
|>	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
||	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
|>	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
|>	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
|>	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
|>	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
|>	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
|>	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
|>	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
|>	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
|>	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
||Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.DSQuotaExceededException): The DiskSpace quota of /dataspace/team7 is exceeded: quota = 10737418240 B = 10 GB but diskspace consumed = 11049082274 B = 10.29 GB
|>	at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyStoragespaceQuota(DirectoryWithQuotaFeature.java:195)
|>	at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:222)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:1147)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:979)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:938)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addBlock(FSDirWriteFileOp.java:504)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.saveAllocatedBlock(FSDirWriteFileOp.java:771)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.storeAllocatedBlock(FSDirWriteFileOp.java:259)
|>	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2714)
|>	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
|>	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
|>	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
|>	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
|>	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
|>	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
|>	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
||	at java.security.AccessController.doPrivileged(Native Method)
|>	at javax.security.auth.Subject.doAs(Subject.java:422)
|>	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
|>	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
|>	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
|>	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
|>	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
|>	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
|>	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
||	at com.sun.proxy.$Proxy13.addBlock(Unknown Source)
||	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:510)
||	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
|>	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
|>	at java.lang.reflect.Method.invoke(Method.java:498)
|>	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
|>	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
|>	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
|>	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
|>	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
||	at com.sun.proxy.$Proxy14.addBlock(Unknown Source)
||	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1078)
||	... 3 more
322,attempt_1667571544727_0239_m_000000_2,,
||Error: org.apache.hadoop.hdfs.protocol.DSQuotaExceededException: The DiskSpace quota of /dataspace/team7 is exceeded: quota = 10737418240 B = 10 GB but diskspace consumed = 11049082274 B = 10.29 GB
|>	at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyStoragespaceQuota(DirectoryWithQuotaFeature.java:195)
|>	at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:222)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:1147)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:979)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:938)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addBlock(FSDirWriteFileOp.java:504)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.saveAllocatedBlock(FSDirWriteFileOp.java:771)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.storeAllocatedBlock(FSDirWriteFileOp.java:259)
|>	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2714)
|>	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
|>	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
|>	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
|>	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
|>	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
|>	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
|>	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
||	at java.security.AccessController.doPrivileged(Native Method)
|>	at javax.security.auth.Subject.doAs(Subject.java:422)
|>	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
|>	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
||	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
|>	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
|>	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
|>	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
|>	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
|>	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
|>	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
|>	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865)
|>	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
|>	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
||Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.DSQuotaExceededException): The DiskSpace quota of /dataspace/team7 is exceeded: quota = 10737418240 B = 10 GB but diskspace consumed = 11049082274 B = 10.29 GB
|>	at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyStoragespaceQuota(DirectoryWithQuotaFeature.java:195)
|>	at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:222)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:1147)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:979)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:938)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addBlock(FSDirWriteFileOp.java:504)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.saveAllocatedBlock(FSDirWriteFileOp.java:771)
|>	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.storeAllocatedBlock(FSDirWriteFileOp.java:259)
|>	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2714)
|>	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
|>	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
|>	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
|>	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
|>	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
|>	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
|>	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
||	at java.security.AccessController.doPrivileged(Native Method)
|>	at javax.security.auth.Subject.doAs(Subject.java:422)
|>	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
|>	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
|>	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
|>	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
|>	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
|>	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
|>	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
||	at com.sun.proxy.$Proxy13.addBlock(Unknown Source)
||	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:510)
||	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
|>	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
|>	at java.lang.reflect.Method.invoke(Method.java:498)
|>	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
|>	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
|>	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
|>	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
|>	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
||	at com.sun.proxy.$Proxy14.addBlock(Unknown Source)
||	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1078)
||	... 3 more
